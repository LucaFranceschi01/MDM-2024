{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "In this session we will take a large corpus of documents and compute some statistics using data streams methods.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Your name here</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">Your e-mail here</font>\n",
    "\n",
    "Date: <font color='blue'>The current date here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file contain lines of dialogue of a set of movies from the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). We will use the file `movie_lines.tsv` which contains the text of the dialogue, about 3 million words in about 300,000 lines of dialogue.\n",
    "\n",
    "During this practice, **we will never load this file in memory.**\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = 'movie_lines.tsv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_by_words` is a [generator](https://wiki.python.org/moin/Generators), that is, a function that behaves as an iterator. This is a common pattern used in stream processing, and in Python is implemented with the `yield` keyword, instead of `return`.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, 'rt', encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split('\\t')\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print('- Read %d words so far' % (counter))\n",
    "                        else:\n",
    "                            print('- Read %d/%d words so far' % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a first pass over the data. Here we will read only the first 30K nouns. Try with a larger limit if your computer is fast, with a lower limit if your computer is slow. Find something that makes one pass take about 30 seconds and use it for development.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'freaky'\n",
      "Current noun '/u'\n",
      "Current noun 'u'\n",
      "Current noun 'other'\n",
      "Current noun 'punishable'\n",
      "Current noun 'old'\n",
      "Current noun 'other'\n",
      "Current noun 'dumb'\n",
      "Current noun 'good'\n",
      "Current noun 'playing'\n",
      "Current noun 'great'\n",
      "Current noun 'correct'\n",
      "Current noun 'beautiful'\n",
      "- Read 10000/30000 words so far\n",
      "Current noun 'true'\n",
      "Current noun 'small'\n",
      "Current noun 'same'\n",
      "Current noun 'strange'\n",
      "Current noun 'good'\n",
      "Current noun 'legal'\n",
      "Current noun 'late'\n",
      "Current noun 'beautiful'\n",
      "Current noun 'crazy'\n",
      "Current noun 'sixteen'\n",
      "Current noun 'fresh'\n",
      "- Read 20000/30000 words so far\n",
      "Current noun 'next'\n",
      "Current noun 'fifty'\n",
      "Current noun 'little'\n",
      "Current noun 'fifteen-hundred'\n",
      "Current noun 'mean'\n",
      "Current noun 'own'\n",
      "Current noun 'paint-happy'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=30000, report_every=10000):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print('Current noun \\'%s\\'' % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** NLTK may complain that you have some missing files. The following commands may help:\n",
    "\n",
    "```python3\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading the entire dataset in main memory, we will use reservoir sampling to determine approximately the top-10 words.\n",
    "\n",
    "**Reservoir sampling**: In reservoir sampling, if we have a reservoir of size S:\n",
    "\n",
    "* We store the first S elements of the stream\n",
    "* When the n<sup>th</sup> element arrives (let's call it X<sub>n</sub>):\n",
    "   * With probability 1 - s/n, we ignore this element.\n",
    "   * With probability s/n, we:\n",
    "      * Discard a random element from the reservoir\n",
    "      * Add element X<sub>n</sub> to the reservoir (calling *add_to_reservoir*)\n",
    "      \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `add_reservoir(reservoir, item, max_size)` that adds an item to the reservoir, maintaining its size. If the reservoir is already of size *max_size*, a random item is selected and evicted *before* adding the item. It is important to evict an old item *before* adding the new item. Use the following skeleton:\n",
    "\n",
    "```python\n",
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    assert(len(reservoir) <= max_reservoir_size)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir: list, item, max_reservoir_size):\n",
    "    if len(reservoir) == max_reservoir_size:\n",
    "        if random.random() < 1/max_reservoir_size:\n",
    "            reservoir[random.choice(range(max_reservoir_size))] = item\n",
    "    else:\n",
    "        reservoir.append(item)\n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to iterate through the file using the reservoir sampling method seen in class. In this function you will decide, for every item, whether to call *add_to_reservoir* or to ignore the item.\n",
    "\n",
    "You can use the following skeleton:\n",
    "\n",
    "```python\n",
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_parts_of_speech(filename, max_words=max_words, report_every=report_every):\n",
    "    \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return (words_read, reservoir)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_parts_of_speech(filename, max_words=max_words, report_every=report_every, parts_of_speech=parts_of_speech):\n",
    "    \n",
    "            add_to_reservoir(reservoir, word, reservoir_size)\n",
    "            words_read += 1\n",
    "\n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the following code:\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "print('Number of items seen    : %d' % items_seen)\n",
    "print('Number of items sampled : %d' % len(reservoir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservoir contains repeated items. You can compute the absolute frequencies of the top 20 using the following code.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 good\n",
      "35 other\n",
      "33 little\n",
      "32 right\n",
      "29 sure\n",
      "26 last\n",
      "23 real\n",
      "23 bad\n",
      "22 sorry\n",
      "22 much\n",
      "21 own\n",
      "19 only\n",
      "19 great\n",
      "18 big\n",
      "17 first\n",
      "14 wrong\n",
      "14 true\n",
      "14 same\n",
      "14 next\n",
      "14 long\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print('%d %s' % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to compute the 20 most frequent items in the reservoir and their relative frequencies, as percentages.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 good (3.80%)\n",
      "35 other (2.33%)\n",
      "33 little (2.20%)\n",
      "32 right (2.13%)\n",
      "29 sure (1.93%)\n",
      "26 last (1.73%)\n",
      "23 real (1.53%)\n",
      "23 bad (1.53%)\n",
      "22 sorry (1.47%)\n",
      "22 much (1.47%)\n",
      "21 own (1.40%)\n",
      "19 only (1.27%)\n",
      "19 great (1.27%)\n",
      "18 big (1.20%)\n",
      "17 first (1.13%)\n",
      "14 wrong (0.93%)\n",
      "14 true (0.93%)\n",
      "14 same (0.93%)\n",
      "14 next (0.93%)\n",
      "14 long (0.93%)\n"
     ]
    }
   ],
   "source": [
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print('{} {} ({:.2%})'.format(absolute_frequency, word, absolute_frequency/reservoir_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see an item C times in the reservoir, you can estimate the item appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). \n",
    "\n",
    "For various sizes of the reservoir, e.g., 50, 100, 500, ..., list the top-5 words and your estimate of their frequency in the entire dataset.\n",
    " \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes about 2-3 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "2     sacred     (4.00%) -- E(sacred|dataset)       = 1200 \n",
      "2     right      (4.00%) -- E(right|dataset)        = 1200 \n",
      "2     little     (4.00%) -- E(little|dataset)       = 1200 \n",
      "2     good       (4.00%) -- E(good|dataset)         = 1200 \n",
      "1     young      (2.00%) -- E(young|dataset)        = 600  \n",
      "1     true       (2.00%) -- E(true|dataset)         = 600  \n",
      "1     top        (2.00%) -- E(top|dataset)          = 600  \n",
      "1     ten        (2.00%) -- E(ten|dataset)          = 600  \n",
      "1     ta         (2.00%) -- E(ta|dataset)           = 600  \n",
      "1     sure       (2.00%) -- E(sure|dataset)         = 600  \n",
      "1     suntan     (2.00%) -- E(suntan|dataset)       = 600  \n",
      "1     steep      (2.00%) -- E(steep|dataset)        = 600  \n",
      "1     sorry      (2.00%) -- E(sorry|dataset)        = 600  \n",
      "1     simple     (2.00%) -- E(simple|dataset)       = 600  \n",
      "1     same       (2.00%) -- E(same|dataset)         = 600  \n",
      "1     real       (2.00%) -- E(real|dataset)         = 600  \n",
      "1     posi-      (2.00%) -- E(posi-|dataset)        = 600  \n",
      "1     playful    (2.00%) -- E(playful|dataset)      = 600  \n",
      "1     partial    (2.00%) -- E(partial|dataset)      = 600  \n",
      "1     own        (2.00%) -- E(own|dataset)          = 600  \n",
      "======================================== TIME: 25.41s ========================================\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "6     good       (6.00%) -- E(good|dataset)         = 1800 \n",
      "3     sure       (3.00%) -- E(sure|dataset)         = 900  \n",
      "3     right      (3.00%) -- E(right|dataset)        = 900  \n",
      "3     own        (3.00%) -- E(own|dataset)          = 900  \n",
      "3     old        (3.00%) -- E(old|dataset)          = 900  \n",
      "3     next       (3.00%) -- E(next|dataset)         = 900  \n",
      "3     little     (3.00%) -- E(little|dataset)       = 900  \n",
      "2     sorry      (2.00%) -- E(sorry|dataset)        = 600  \n",
      "2     perfect    (2.00%) -- E(perfect|dataset)      = 600  \n",
      "2     other      (2.00%) -- E(other|dataset)        = 600  \n",
      "2     first      (2.00%) -- E(first|dataset)        = 600  \n",
      "2     bad        (2.00%) -- E(bad|dataset)          = 600  \n",
      "2     afraid     (2.00%) -- E(afraid|dataset)       = 600  \n",
      "1     young      (1.00%) -- E(young|dataset)        = 300  \n",
      "1     y'know     (1.00%) -- E(y'know|dataset)       = 300  \n",
      "1     worth      (1.00%) -- E(worth|dataset)        = 300  \n",
      "1     wonderful  (1.00%) -- E(wonderful|dataset)    = 300  \n",
      "1     willing    (1.00%) -- E(willing|dataset)      = 300  \n",
      "1     unobserved (1.00%) -- E(unobserved|dataset)   = 300  \n",
      "1     unnecessary (1.00%) -- E(unnecessary|dataset)  = 300  \n",
      "======================================== TIME: 22.90s ========================================\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "16    good       (3.20%) -- E(good|dataset)         = 960  \n",
      "12    right      (2.40%) -- E(right|dataset)        = 720  \n",
      "11    other      (2.20%) -- E(other|dataset)        = 660  \n",
      "11    little     (2.20%) -- E(little|dataset)       = 660  \n",
      "10    sure       (2.00%) -- E(sure|dataset)         = 600  \n",
      "9     such       (1.80%) -- E(such|dataset)         = 540  \n",
      "9     ready      (1.80%) -- E(ready|dataset)        = 540  \n",
      "9     own        (1.80%) -- E(own|dataset)          = 540  \n",
      "8     big        (1.60%) -- E(big|dataset)          = 480  \n",
      "8     bad        (1.60%) -- E(bad|dataset)          = 480  \n",
      "7     only       (1.40%) -- E(only|dataset)         = 420  \n",
      "7     new        (1.40%) -- E(new|dataset)          = 420  \n",
      "7     much       (1.40%) -- E(much|dataset)         = 420  \n",
      "7     last       (1.40%) -- E(last|dataset)         = 420  \n",
      "6     stupid     (1.20%) -- E(stupid|dataset)       = 360  \n",
      "6     dead       (1.20%) -- E(dead|dataset)         = 360  \n",
      "5     wrong      (1.00%) -- E(wrong|dataset)        = 300  \n",
      "5     whole      (1.00%) -- E(whole|dataset)        = 300  \n",
      "5     true       (1.00%) -- E(true|dataset)         = 300  \n",
      "5     normal     (1.00%) -- E(normal|dataset)       = 300  \n",
      "======================================== TIME: 23.07s ========================================\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "35    good       (3.50%) -- E(good|dataset)         = 1050 \n",
      "24    right      (2.40%) -- E(right|dataset)        = 720  \n",
      "22    sure       (2.20%) -- E(sure|dataset)         = 660  \n",
      "21    little     (2.10%) -- E(little|dataset)       = 630  \n",
      "19    other      (1.90%) -- E(other|dataset)        = 570  \n",
      "16    own        (1.60%) -- E(own|dataset)          = 480  \n",
      "14    true       (1.40%) -- E(true|dataset)         = 420  \n",
      "13    sorry      (1.30%) -- E(sorry|dataset)        = 390  \n",
      "13    bad        (1.30%) -- E(bad|dataset)          = 390  \n",
      "12    much       (1.20%) -- E(much|dataset)         = 360  \n",
      "12    great      (1.20%) -- E(great|dataset)        = 360  \n",
      "11    real       (1.10%) -- E(real|dataset)         = 330  \n",
      "11    big        (1.10%) -- E(big|dataset)          = 330  \n",
      "10    wrong      (1.00%) -- E(wrong|dataset)        = 300  \n",
      "10    only       (1.00%) -- E(only|dataset)         = 300  \n",
      "10    next       (1.00%) -- E(next|dataset)         = 300  \n",
      "10    many       (1.00%) -- E(many|dataset)         = 300  \n",
      "10    last       (1.00%) -- E(last|dataset)         = 300  \n",
      "10    first      (1.00%) -- E(first|dataset)        = 300  \n",
      "10    fine       (1.00%) -- E(fine|dataset)         = 300  \n",
      "======================================== TIME: 22.32s ========================================\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "63    good       (4.20%) -- E(good|dataset)         = 1260 \n",
      "33    other      (2.20%) -- E(other|dataset)        = 660  \n",
      "32    little     (2.13%) -- E(little|dataset)       = 640  \n",
      "31    right      (2.07%) -- E(right|dataset)        = 620  \n",
      "28    sure       (1.87%) -- E(sure|dataset)         = 560  \n",
      "24    real       (1.60%) -- E(real|dataset)         = 480  \n",
      "24    last       (1.60%) -- E(last|dataset)         = 480  \n",
      "22    sorry      (1.47%) -- E(sorry|dataset)        = 440  \n",
      "22    much       (1.47%) -- E(much|dataset)         = 440  \n",
      "22    bad        (1.47%) -- E(bad|dataset)          = 440  \n",
      "21    own        (1.40%) -- E(own|dataset)          = 420  \n",
      "19    great      (1.27%) -- E(great|dataset)        = 380  \n",
      "18    only       (1.20%) -- E(only|dataset)         = 360  \n",
      "17    big        (1.13%) -- E(big|dataset)          = 340  \n",
      "16    wrong      (1.07%) -- E(wrong|dataset)        = 320  \n",
      "16    first      (1.07%) -- E(first|dataset)        = 320  \n",
      "14    true       (0.93%) -- E(true|dataset)         = 280  \n",
      "14    same       (0.93%) -- E(same|dataset)         = 280  \n",
      "14    next       (0.93%) -- E(next|dataset)         = 280  \n",
      "14    long       (0.93%) -- E(long|dataset)         = 280  \n",
      "======================================== TIME: 22.75s ========================================\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "109   good       (3.63%) -- E(good|dataset)         = 1090 \n",
      "74    little     (2.47%) -- E(little|dataset)       = 740  \n",
      "65    right      (2.17%) -- E(right|dataset)        = 650  \n",
      "63    u          (2.10%) -- E(u|dataset)            = 630  \n",
      "56    other      (1.87%) -- E(other|dataset)        = 560  \n",
      "51    sure       (1.70%) -- E(sure|dataset)         = 510  \n",
      "47    last       (1.57%) -- E(last|dataset)         = 470  \n",
      "45    real       (1.50%) -- E(real|dataset)         = 450  \n",
      "45    bad        (1.50%) -- E(bad|dataset)          = 450  \n",
      "44    much       (1.47%) -- E(much|dataset)         = 440  \n",
      "42    dead       (1.40%) -- E(dead|dataset)         = 420  \n",
      "41    wrong      (1.37%) -- E(wrong|dataset)        = 410  \n",
      "40    sorry      (1.33%) -- E(sorry|dataset)        = 400  \n",
      "40    big        (1.33%) -- E(big|dataset)          = 400  \n",
      "35    few        (1.17%) -- E(few|dataset)          = 350  \n",
      "33    only       (1.10%) -- E(only|dataset)         = 330  \n",
      "30    own        (1.00%) -- E(own|dataset)          = 300  \n",
      "29    nice       (0.97%) -- E(nice|dataset)         = 290  \n",
      "29    first      (0.97%) -- E(first|dataset)        = 290  \n",
      "28    old        (0.93%) -- E(old|dataset)          = 280  \n",
      "======================================== TIME: 22.83s ========================================\n"
     ]
    }
   ],
   "source": [
    "sizes = [50, 100, 500, 1000, 1500, 3000, 6000]\n",
    "\n",
    "for reservoir_size in sizes:\n",
    "    start = time.time()\n",
    "    (items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = reservoir.count(item)\n",
    "\n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "    \n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        print('{:<5d} {:<11s} ({:<2.2%}) -- {:<23s} = {:<5d}'.format(absolute_frequency, word, absolute_frequency/reservoir_size, f'E({word}|dataset)', round(absolute_frequency*items_seen/reservoir_size)))\n",
    "\n",
    "    end = time.time()\n",
    "    print('======================================== TIME: {:.2f}s ========================================'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find by trial and error, and include in your report, the minimum reservoir size you need to have somewhat stable results (e.g., the same top-3 words in two consecutive runs of the algorithm).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the number of distinct words without creating a dictionary or hash table, but instead, we will use the Flajolet-Martin probabilistic counting method.\n",
    "\n",
    "**Flajolet-Martin probabilistic counting**:\n",
    "\n",
    "* For several passes\n",
    "   * Create hash funcion h\n",
    "   * For every element *u* in the stream:\n",
    "      * Compute hash value *h(u)*\n",
    "      * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "      * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "   * Add *2<sup>R</sup>* as an estimate for the number of distinct elements *u* seen\n",
    "* The final estimate is the average or the median of the estimates found in each pass\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to count trailing zeroes in the binary representation of a number.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to generate a random hash function. Note this generates a function, so you can do `hash_function = random_hash_function()` and then call `hash_function(x)` to compute the hash value of `x`. \n",
    "\n",
    "We want to make sure each hash is different, so we will create each hash function with a different [salt](https://en.wikipedia.org/wiki/Salt_(cryptography)), which is an additional input that we will take using a good random string generator from the [secrets](https://docs.python.org/3/library/secrets.html) library.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform *number_of_passes* passes over the file, reading the entire file on each pass (we don't use the reservoir in this part). In each pass, create a new hash function and use it to hash userids. Keep the maximum number of trailing zeroes seen in the hash value of a userid. \n",
    "\n",
    "```python\n",
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Leave this code as-is\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m* Average of estimates: \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m statistics\u001b[38;5;241m.\u001b[39mmean(\u001b[43mestimates\u001b[49m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m* Median  of estimates: \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m statistics\u001b[38;5;241m.\u001b[39mmedian(estimates))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimates' is not defined"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase the limit of words to read (but do not use more than 5 minutes of computing time), and perform the 10 passes. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Compute the median of average estimates in 3 separate runs of your algorithm; each run should do 10 passes over the file. Repeat this for nouns (POS_NOUN), adjectives (POS_ADJECTIVE), and verbs (POS_VERB). Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "Remember to read the section on \"delivering your code\" in the [course evaluation guidelines](https://github.com/chatox/data-mining-course/blob/master/upf/upf-evaluation.md).\n",
    "\n",
    "Deliver a zip file containing:\n",
    "\n",
    "* This notebook\n",
    "\n",
    "## Extra points available\n",
    "\n",
    "For more learning and extra points, notice that the number of **distinct** words in a corpus, as a function of the **total** number of words in the corpus, follows an empirical law known as [Heap's Law](https://en.wikipedia.org/wiki/Heaps%27_law).\n",
    "\n",
    "Repeat the probabilistic counting experiment for various values of `max_word` and plot the total number of words read versus the number of distinct words (remember to label axes). Check if it follows Heap's law for nouns, verbs, and adjectives.\n",
    "\n",
    "Please note that using probabilistic counting means a substantial amount of noise will be introduced and perhaps the Heap's law will not be clear in your plot.\n",
    "\n",
    "**Note:** if you go for the extra points, add ``<font size=\"+2\" color=\"blue\">Additional results: Heap's law</font>`` at the top of your notebook. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
