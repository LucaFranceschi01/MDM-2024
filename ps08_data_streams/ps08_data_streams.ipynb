{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "In this session we will take a large corpus of documents and compute some statistics using data streams methods.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Luca Franceschi</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">luca.franceschi01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">4/12/2024</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file contain lines of dialogue of a set of movies from the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). We will use the file `movie_lines.tsv` which contains the text of the dialogue, about 3 million words in about 300,000 lines of dialogue.\n",
    "\n",
    "During this practice, **we will never load this file in memory.**\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = 'movie_lines.tsv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_by_words` is a [generator](https://wiki.python.org/moin/Generators), that is, a function that behaves as an iterator. This is a common pattern used in stream processing, and in Python is implemented with the `yield` keyword, instead of `return`.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "\n",
    "    # Open the input file\n",
    "    with gzip.open(filename, 'rt', encoding='utf8') as file:\n",
    "\n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "\n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "\n",
    "            elements = line.split('\\t')\n",
    "\n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "\n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "\n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "\n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print('- Read %d words so far' % (counter))\n",
    "                        else:\n",
    "                            print('- Read %d/%d words so far' % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a first pass over the data. Here we will read only the first 30K nouns. Try with a larger limit if your computer is fast, with a lower limit if your computer is slow. Find something that makes one pass take about 30 seconds and use it for development.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'male-dominated'\n",
      "Current noun 'newsreel'\n",
      "Current noun 'odd'\n",
      "Current noun 'german'\n",
      "Current noun 'sure'\n",
      "Current noun 'steady'\n",
      "Current noun 'hot'\n",
      "Current noun 'whole'\n",
      "Current noun 'ok'\n",
      "Current noun 'joint'\n",
      "Current noun 'creative'\n",
      "Current noun 'middle'\n",
      "- Read 10000/30000 words so far\n",
      "Current noun 'good'\n",
      "Current noun 'soft'\n",
      "Current noun 'important'\n",
      "Current noun 'long'\n",
      "Current noun 'suicide'\n",
      "Current noun 'gracious'\n",
      "Current noun 'small'\n",
      "Current noun 'n-a'\n",
      "Current noun 'break-'\n",
      "Current noun 'easy'\n",
      "Current noun 'psychic'\n",
      "Current noun 'top'\n",
      "Current noun 'old'\n",
      "Current noun 'wrong'\n",
      "- Read 20000/30000 words so far\n",
      "Current noun 'real'\n",
      "Current noun 'thirty'\n",
      "Current noun 'real'\n",
      "Current noun 'crap'\n",
      "Current noun 'first'\n",
      "Current noun 'right'\n",
      "Current noun 'quiet'\n",
      "Current noun 'early'\n",
      "Current noun 'sure'\n",
      "Current noun 'weird'\n",
      "Current noun 'crazy'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "for word in read_by_parts_of_speech(\n",
    "    INPUT_FILE, [POS_ADJECTIVE], max_words=30000, report_every=10000\n",
    "):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print('Current noun \\'%s\\'' % (word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** NLTK may complain that you have some missing files. The following commands may help:\n",
    "\n",
    "```python3\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading the entire dataset in main memory, we will use reservoir sampling to determine approximately the top-10 words.\n",
    "\n",
    "**Reservoir sampling**: In reservoir sampling, if we have a reservoir of size S:\n",
    "\n",
    "* We store the first S elements of the stream\n",
    "* When the n<sup>th</sup> element arrives (let's call it X<sub>n</sub>):\n",
    "   * With probability 1 - s/n, we ignore this element.\n",
    "   * With probability s/n, we:\n",
    "      * Discard a random element from the reservoir\n",
    "      * Add element X<sub>n</sub> to the reservoir (calling *add_to_reservoir*)\n",
    "      \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `add_reservoir(reservoir, item, max_size)` that adds an item to the reservoir, maintaining its size. If the reservoir is already of size *max_size*, a random item is selected and evicted *before* adding the item. It is important to evict an old item *before* adding the new item. Use the following skeleton:\n",
    "\n",
    "```python\n",
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    assert(len(reservoir) <= max_reservoir_size)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir: list, item, max_reservoir_size):\n",
    "    if len(reservoir) == max_reservoir_size:\n",
    "        if random.random() < 1 / max_reservoir_size:\n",
    "            reservoir[random.choice(range(max_reservoir_size))] = item\n",
    "    else:\n",
    "        reservoir.append(item)\n",
    "    assert len(reservoir) <= max_reservoir_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to iterate through the file using the reservoir sampling method seen in class. In this function you will decide, for every item, whether to call *add_to_reservoir* or to ignore the item.\n",
    "\n",
    "You can use the following skeleton:\n",
    "\n",
    "```python\n",
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_parts_of_speech(filename, max_words=max_words, report_every=report_every):\n",
    "    \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return (words_read, reservoir)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(\n",
    "    filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1\n",
    "):\n",
    "    reservoir = []\n",
    "\n",
    "    words_read = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(\n",
    "        filename,\n",
    "        max_words=max_words,\n",
    "        report_every=report_every,\n",
    "        parts_of_speech=parts_of_speech,\n",
    "    ):\n",
    "\n",
    "        add_to_reservoir(reservoir, word, reservoir_size)\n",
    "        words_read += 1\n",
    "\n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the following code:\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(\n",
    "    INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000\n",
    ")\n",
    "\n",
    "print('Number of items seen    : %d' % items_seen)\n",
    "print('Number of items sampled : %d' % len(reservoir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservoir contains repeated items. You can compute the absolute frequencies of the top 20 using the following code.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 good\n",
      "32 other\n",
      "32 little\n",
      "31 right\n",
      "29 sure\n",
      "24 last\n",
      "23 sorry\n",
      "23 bad\n",
      "22 real\n",
      "22 much\n",
      "20 own\n",
      "20 great\n",
      "18 only\n",
      "18 big\n",
      "17 first\n",
      "15 true\n",
      "14 wrong\n",
      "14 same\n",
      "14 next\n",
      "14 long\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted(\n",
    "    [(frequency, word) for word, frequency in freq.items()], reverse=True\n",
    ")[:20]\n",
    "\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print('%d %s' % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to compute the 20 most frequent items in the reservoir and their relative frequencies, as percentages.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 good (4.00%)\n",
      "32 other (2.13%)\n",
      "32 little (2.13%)\n",
      "31 right (2.07%)\n",
      "29 sure (1.93%)\n",
      "24 last (1.60%)\n",
      "23 sorry (1.53%)\n",
      "23 bad (1.53%)\n",
      "22 real (1.47%)\n",
      "22 much (1.47%)\n",
      "20 own (1.33%)\n",
      "20 great (1.33%)\n",
      "18 only (1.20%)\n",
      "18 big (1.20%)\n",
      "17 first (1.13%)\n",
      "15 true (1.00%)\n",
      "14 wrong (0.93%)\n",
      "14 same (0.93%)\n",
      "14 next (0.93%)\n",
      "14 long (0.93%)\n"
     ]
    }
   ],
   "source": [
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\n",
    "        '{} {} ({:.2%})'.format(\n",
    "            absolute_frequency, word, absolute_frequency / reservoir_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see an item C times in the reservoir, you can estimate the item appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). \n",
    "\n",
    "For various sizes of the reservoir, e.g., 50, 100, 500, ..., list the top-5 words and your estimate of their frequency in the entire dataset.\n",
    " \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes about 2-3 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir size = 50 (0.00%)\n",
      "- Read 10000/50000 words so far\n",
      "- Read 20000/50000 words so far\n",
      "- Read 30000/50000 words so far\n",
      "- Read 40000/50000 words so far\n",
      "- Read 50000/50000 words so far\n",
      "word                │ abs_freq │ rel_freq │ E(abs_freq|dataset)\n",
      "────────────────────┼──────────┼──────────┼─────────────────────\n",
      "good                │    3     │  6.00%   │        180000       \n",
      "simple              │    2     │  4.00%   │        120000       \n",
      "same                │    2     │  4.00%   │        120000       \n",
      "right               │    2     │  4.00%   │        120000       \n",
      "which               │    1     │  2.00%   │        60000        \n",
      "========================= TIME: 38.08s =========================\n",
      "Reservoir size = 100 (0.00%)\n",
      "- Read 10000/50000 words so far\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReservoir size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreservoir_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreservoir_size\u001b[38;5;241m/\u001b[39mdataset_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m (_, reservoir) \u001b[38;5;241m=\u001b[39m \u001b[43mreservoir_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mINPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPOS_ADJECTIVE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreservoir_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m freq \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reservoir:\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mreservoir_sampling\u001b[0;34m(filename, parts_of_speech, reservoir_size, max_words, report_every)\u001b[0m\n\u001b[1;32m      4\u001b[0m reservoir \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m words_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_by_parts_of_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparts_of_speech\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparts_of_speech\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_to_reservoir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreservoir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreservoir_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwords_read\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mread_by_parts_of_speech\u001b[0;34m(filename, parts_of_speech, max_words, report_every)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(text):\n\u001b[0;32m---> 31\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m [part[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m part[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m parts_of_speech]:\n\u001b[1;32m     34\u001b[0m         counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/repos/MDM-2024/.venv/lib64/python3.13/site-packages/nltk/tag/__init__.py:169\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repos/MDM-2024/.venv/lib64/python3.13/site-packages/nltk/tag/__init__.py:126\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/repos/MDM-2024/.venv/lib64/python3.13/site-packages/nltk/tag/perceptron.py:201\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    200\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 201\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    204\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/Documents/repos/MDM-2024/.venv/lib64/python3.13/site-packages/nltk/tag/perceptron.py:83\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     81\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sizes = [\n",
    "    50,\n",
    "    100,\n",
    "    300,\n",
    "    500,\n",
    "    1000,\n",
    "    1500,\n",
    "    2999,\n",
    "    3000,\n",
    "    3001,\n",
    "    6000,\n",
    "    12000,\n",
    "    15000,\n",
    "    30000,\n",
    "    50000,\n",
    "]\n",
    "top_n = 5\n",
    "dataset_size = 3000000  # from the problem statement\n",
    "max_words = 50000\n",
    "\n",
    "for reservoir_size in sizes:\n",
    "    print(f'Reservoir size = {reservoir_size} ({reservoir_size/dataset_size:.2%})')\n",
    "    start = time.time()\n",
    "    (_, reservoir) = reservoir_sampling(\n",
    "        INPUT_FILE,\n",
    "        [POS_ADJECTIVE],\n",
    "        reservoir_size,\n",
    "        max_words=max_words,\n",
    "        report_every=10000,\n",
    "    )\n",
    "\n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = reservoir.count(item)\n",
    "    most_frequent_items = sorted(\n",
    "        [(frequency, word) for word, frequency in freq.items()], reverse=True\n",
    "    )[:top_n]\n",
    "\n",
    "    print(\n",
    "        '{:<19s} \\u2502 {:<8s} \\u2502 {:<8s} \\u2502 {:<19s}'.format(\n",
    "            'word', 'abs_freq', 'rel_freq', 'E(abs_freq|dataset)'\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        '{:\\u2500<20s}\\u253C{:\\u2500<10s}\\u253C{:\\u2500<10s}\\u253C{:\\u2500<21s}'.format(\n",
    "            '', '', '', ''\n",
    "        )\n",
    "    )\n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        print(\n",
    "            '{:<19s} \\u2502 {:^8d} \\u2502 {:^8.2%} \\u2502 {:^20d}'.format(\n",
    "                word,\n",
    "                absolute_frequency,\n",
    "                absolute_frequency / reservoir_size,\n",
    "                round(absolute_frequency * dataset_size / reservoir_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f'========================= TIME: {end - start:.2f}s ========================='\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find by trial and error, and include in your report, the minimum reservoir size you need to have somewhat stable results (e.g., the same top-3 words in two consecutive runs of the algorithm).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal reservoir size may depend on the max_words value, in this case it is about 10% of the total words read (around 3000 elements in the reservoir) the results tend to stabilize. However for even more stable results bigger reservoir sizes may be used. This choice typically depends on the system specifications of the device running this code and how many samples it can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the number of distinct words without creating a dictionary or hash table, but instead, we will use the Flajolet-Martin probabilistic counting method.\n",
    "\n",
    "**Flajolet-Martin probabilistic counting**:\n",
    "\n",
    "* For several passes\n",
    "   * Create hash funcion h\n",
    "   * For every element *u* in the stream:\n",
    "      * Compute hash value *h(u)*\n",
    "      * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "      * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "   * Add *2<sup>R</sup>* as an estimate for the number of distinct elements *u* seen\n",
    "* The final estimate is the average or the median of the estimates found in each pass\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to count trailing zeroes in the binary representation of a number.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to generate a random hash function. Note this generates a function, so you can do `hash_function = random_hash_function()` and then call `hash_function(x)` to compute the hash value of `x`. \n",
    "\n",
    "We want to make sure each hash is different, so we will create each hash function with a different [salt](https://en.wikipedia.org/wiki/Salt_(cryptography)), which is an additional input that we will take using a good random string generator from the [secrets](https://docs.python.org/3/library/secrets.html) library.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform *number_of_passes* passes over the file, reading the entire file on each pass (we don't use the reservoir in this part). In each pass, create a new hash function and use it to hash userids. Keep the maximum number of trailing zeroes seen in the hash value of a userid. \n",
    "\n",
    "```python\n",
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 1: 4096 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 2: 1024 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 3: 1024 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 4: 16384 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 5: 2048 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 5\n",
    "\n",
    "\n",
    "def probabilistic_counting(\n",
    "    number_of_passes, filename, parts_of_speech, max_words=30000, report_every=10000\n",
    "):\n",
    "    estimates = []\n",
    "\n",
    "    for i in range(number_of_passes):\n",
    "        # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "        hash_function = random_hash_function()\n",
    "        estimate = -1\n",
    "        for word in read_by_parts_of_speech(\n",
    "            filename, parts_of_speech, max_words, report_every\n",
    "        ):\n",
    "            estimate = max(estimate, count_trailing_zeroes(hash_function(word)))\n",
    "        estimate = 2**estimate\n",
    "        estimates.append(estimate)\n",
    "        print('Estimate on pass %d: %d distinct words' % (i + 1, estimate))\n",
    "    return estimates\n",
    "\n",
    "\n",
    "estimates = probabilistic_counting(number_of_passes, INPUT_FILE, [POS_ADJECTIVE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 4915.2\n",
      "* Median  of estimates: 2048.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print('* Average of estimates: %.1f' % statistics.mean(estimates))\n",
    "print('* Median  of estimates: %.1f' % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase the limit of words to read (but do not use more than 5 minutes of computing time), and perform the 10 passes. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Compute the median of average estimates in 3 separate runs of your algorithm; each run should do 10 passes over the file. Repeat this for nouns (POS_NOUN), adjectives (POS_ADJECTIVE), and verbs (POS_VERB). Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting NN part of speech\n",
      "Estimate on pass 1: 8192 distinct words\n",
      "Estimate on pass 2: 4096 distinct words\n",
      "Estimate on pass 3: 2048 distinct words\n",
      "Estimate on pass 4: 2048 distinct words\n",
      "Estimate on pass 5: 4096 distinct words\n",
      "Estimate on pass 6: 4096 distinct words\n",
      "Estimate on pass 7: 4096 distinct words\n",
      "Estimate on pass 8: 131072 distinct words\n",
      "Estimate on pass 9: 32768 distinct words\n",
      "Estimate on pass 10: 8192 distinct words\n",
      "============ TIME: 91.74s ============\n",
      "Counting JJ part of speech\n",
      "Estimate on pass 1: 1024 distinct words\n",
      "Estimate on pass 2: 2048 distinct words\n",
      "Estimate on pass 3: 2048 distinct words\n",
      "Estimate on pass 4: 2048 distinct words\n",
      "Estimate on pass 5: 16384 distinct words\n",
      "Estimate on pass 6: 4096 distinct words\n",
      "Estimate on pass 7: 2048 distinct words\n",
      "Estimate on pass 8: 4096 distinct words\n",
      "Estimate on pass 9: 8192 distinct words\n",
      "Estimate on pass 10: 2048 distinct words\n",
      "============ TIME: 223.92s ============\n",
      "Counting VB part of speech\n",
      "Estimate on pass 1: 4096 distinct words\n",
      "Estimate on pass 2: 4096 distinct words\n",
      "Estimate on pass 3: 8192 distinct words\n",
      "Estimate on pass 4: 512 distinct words\n",
      "Estimate on pass 5: 32768 distinct words\n",
      "Estimate on pass 6: 512 distinct words\n",
      "Estimate on pass 7: 512 distinct words\n",
      "Estimate on pass 8: 16384 distinct words\n",
      "Estimate on pass 9: 2048 distinct words\n",
      "Estimate on pass 10: 1024 distinct words\n",
      "============ TIME: 159.30s ============\n"
     ]
    }
   ],
   "source": [
    "parts_of_speech = [(POS_NOUN), (POS_ADJECTIVE), (POS_VERB)]\n",
    "number_of_passes = 10\n",
    "\n",
    "estimates = {}\n",
    "\n",
    "for part_of_speech in parts_of_speech:\n",
    "    print(f'Counting {part_of_speech} part of speech')\n",
    "    start = time.time()\n",
    "    estimates[part_of_speech] = probabilistic_counting(\n",
    "        number_of_passes, INPUT_FILE, part_of_speech, report_every=-1\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f'============ TIME: {end - start:.2f}s ============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for NN part of speech\n",
      "* Average of estimates: 20070.4\n",
      "* Median  of estimates: 4096.0\n",
      "Statistics for JJ part of speech\n",
      "* Average of estimates: 4403.2\n",
      "* Median  of estimates: 2048.0\n",
      "Statistics for VB part of speech\n",
      "* Average of estimates: 7014.4\n",
      "* Median  of estimates: 3072.0\n"
     ]
    }
   ],
   "source": [
    "for key in estimates:\n",
    "    print(f'Statistics for {key} part of speech')\n",
    "    print('* Average of estimates: %.1f' % statistics.mean(estimates[key]))\n",
    "    print('* Median  of estimates: %.1f' % statistics.median(estimates[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say that the median is more appropiate since there is some small chance that we get a hash function that maps a single word to a number that has a ton of trailing zeros, causing the estimate of a single pass to be a couple of orders of magnitude higher, driving the average to a much higher value than expected. This happens in our run with the NN (nouns) part of speech. We can say that the median is more stable in this probabilistic counting situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "Remember to read the section on \"delivering your code\" in the [course evaluation guidelines](https://github.com/chatox/data-mining-course/blob/master/upf/upf-evaluation.md).\n",
    "\n",
    "Deliver a zip file containing:\n",
    "\n",
    "* This notebook\n",
    "\n",
    "## Extra points available\n",
    "\n",
    "For more learning and extra points, notice that the number of **distinct** words in a corpus, as a function of the **total** number of words in the corpus, follows an empirical law known as [Heap's Law](https://en.wikipedia.org/wiki/Heaps%27_law).\n",
    "\n",
    "Repeat the probabilistic counting experiment for various values of `max_word` and plot the total number of words read versus the number of distinct words (remember to label axes). Check if it follows Heap's law for nouns, verbs, and adjectives.\n",
    "\n",
    "Please note that using probabilistic counting means a substantial amount of noise will be introduced and perhaps the Heap's law will not be clear in your plot.\n",
    "\n",
    "**Note:** if you go for the extra points, add ``<font size=\"+2\" color=\"blue\">Additional results: Heap's law</font>`` at the top of your notebook. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
