{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 03: Find near-duplicates using shingling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"blue\">Additional results: various ngram sizes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Luca Franceschi</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">luca.franceschi01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">16/10/2024</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP AS-IS\n",
    "\n",
    "# Input file\n",
    "INPUT_FILENAME = \"CovidLockdownCatalonia.json.gz\"\n",
    "\n",
    "# Array for storing messages\n",
    "messages = []\n",
    "\n",
    "# IMPORTANT:\n",
    "# 1. Set this to 1000 during development\n",
    "# 2. Set this to 10000 once you have completed the development, and answer part 5 with 10000\n",
    "# 3. Set this back to 1000 for delivering your code\n",
    "MAX_MESSAGES = 1000\n",
    "\n",
    "with gzip.open(INPUT_FILENAME, \"rt\", encoding=\"utf-8\") as input_file:\n",
    "    \n",
    "    messages_read = 0\n",
    "    for line in input_file:\n",
    "            \n",
    "        # Read message\n",
    "        tweet = json.loads(line)\n",
    "\n",
    "        # Keep only messages in Catalan\n",
    "        if tweet[\"lang\"] == \"ca\":\n",
    "            \n",
    "            messages_read += 1\n",
    "            \n",
    "            if messages_read <= MAX_MESSAGES:\n",
    "                author = tweet[\"user\"][\"screen_name\"]\n",
    "                message = tweet[\"full_text\"]\n",
    "                messages.append(message)\n",
    "\n",
    "print(\"Read %d documents\" % len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for function \"jaccard_similarity\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(v1: tuple, v2: tuple):\n",
    "    s1 = set(v1)\n",
    "    s2 = set(v2)\n",
    "    return len(s1.intersection(s2)) / max(1, len(s1.union(s2))) # this max really only applies when both are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code testing \"jaccard_similarity\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    [[1, 2], [1]],\n",
    "    [[1, 2, 3, 4], [1, 2, 3]],\n",
    "    [[], []],\n",
    "    [[1, 2], [1, 2]],\n",
    "]\n",
    "\n",
    "for l1, l2 in examples:\n",
    "    print(f'Jaccard similariry of {l1} and {l2} is {jaccard_similarity(l1, l2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for function \"clean\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt: str, mode:str=''):\n",
    "    '''\n",
    "    On normal mode it outputs the last version (fully cleaned) string.\n",
    "\n",
    "    On testing mode it outputs all the versions of the string and all the changes done to it so\n",
    "    we can test if it is performing well.\n",
    "    '''\n",
    "    versions = [txt]\n",
    "\n",
    "    # 1. Removing \"RT \" prefixes\n",
    "    versions.append(re.sub(r'^RT ', '', txt))\n",
    "\n",
    "    # 2. Converting to lowercase\n",
    "    versions.append(versions[-1].lower())\n",
    "\n",
    "    # 3. Romanizing text, replacing \"Ñ\" by \"n\", \"ñ\" by \"n\", \"ó\" by \"o\", \"à\" by \"a\", and so on\n",
    "    versions.append(unidecode(versions[-1]))\n",
    "\n",
    "    # 4 \"l·l\" by \"ll\" (which is not done with unidecode by default)\n",
    "    versions.append(re.sub(r'l\\*l', 'll', versions[-1]))\n",
    "\n",
    "    # 5. Removing URLs, both \"http\" and \"https\" ones.\n",
    "    versions.append(re.sub(r'https?:\\/\\/[\\S]*', '', versions[-1]))\n",
    "\n",
    "    # 6. Removing anything that remains that is not a letter or digit\n",
    "    versions.append(re.sub(r'[^a-zA-Z0-9 ]', ' ', versions[-1]))\n",
    "\n",
    "    # 7. Changing double spaces to single spaces.\n",
    "    versions.append(re.sub(r' +', ' ', versions[-1]))\n",
    "\n",
    "    # 8. Removing spaces at the beginning and spaces at the end with the `strip()` function.\n",
    "    versions.append(versions[-1].strip())\n",
    "\n",
    "    if mode == 'testing':\n",
    "        # Which changes did this function perform\n",
    "        changes = []\n",
    "        for i in range(len(versions)-1):\n",
    "            if versions[i] != versions[i+1]:\n",
    "                changes.append(i+1)\n",
    "\n",
    "        return versions, set(changes)\n",
    "    \n",
    "    return versions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code testing function \"clean\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare first iteration\n",
    "i=0\n",
    "msg = messages[i]\n",
    "all_changes = set(range(1,9))\n",
    "txt_changes = set()\n",
    "\n",
    "while txt_changes != all_changes:\n",
    "    # Perform current iteration\n",
    "    txt_cleaned, tmp = clean(msg, 'testing')\n",
    "    print(i, tmp, txt_cleaned[-1])\n",
    "\n",
    "    # Prepare next iteration\n",
    "    txt_changes = txt_changes.union(tmp)\n",
    "    i += 1\n",
    "    msg = messages[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see in the above loop that text 23 has all the modifications so let's check that out\n",
    "clean(messages[23], 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement an n-gram extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code implementing function \"ngrams(text,size)\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TOKEN_LENGTH = 3\n",
    "\n",
    "def ngrams(text, size):\n",
    "    tokens = clean(text).split()\n",
    "    ngrams = []\n",
    "\n",
    "    # First we remove the tokens that have length smaller than MIN_TOKEN_LENGTH\n",
    "    tokens = [tok for tok in tokens if len(tok) >= MIN_TOKEN_LENGTH]\n",
    "\n",
    "    # Then we calculate the ngrams\n",
    "    for i in range(len(tokens)-(size-1)):\n",
    "        ngrams.append(tokens[i:i+size])\n",
    "\n",
    "    # Make ngrams a string again\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code testing function \"ngrams\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages[990])\n",
    "print(ngrams(messages[990], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estimation of brute force all-pairs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE AS-IS\n",
    "\n",
    "def time_brute_force_similarities(messages, limit, ngram_size):\n",
    "    if limit > len(messages):\n",
    "        raise ValueError(\"Limit should be less than or equal than the number of messages\")\n",
    "        \n",
    "    # Start a timer\n",
    "    start = timer()\n",
    "\n",
    "    # Iterate through document identifiers\n",
    "    for docid1 in range(np.min([len(messages), limit])):\n",
    "\n",
    "        # Clean document 1 and extract ngrams\n",
    "        doc1 = clean(messages[docid1])\n",
    "        ngrams1 = ngrams(doc1, ngram_size)\n",
    "\n",
    "        # Iterate through document identifiers larger than doc2\n",
    "        for docid2 in range(docid1+1, np.min([len(messages), limit])):\n",
    "                         \n",
    "            # Clean document 2 and extract ngrams\n",
    "            doc2 = clean(messages[docid2])\n",
    "            ngrams2 = ngrams(doc2, ngram_size)\n",
    "\n",
    "            # Compute similarity\n",
    "            similarity = jaccard_similarity(ngrams1, ngrams2)\n",
    "\n",
    "    end = timer()\n",
    "    return(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for generating the requested plot. Remember to add a title and label for the x and y axis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [range(0, len(messages)+1, 200), []]\n",
    "for i in times[0]:\n",
    "    times[1].append(time_brute_force_similarities(messages, i, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(x):\n",
    "    times_2 = np.array(times[0])**2\n",
    "    return x**2/np.max(times_2)*times[1][-1] # Normalizes according to longest time in list of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title('Brute force time complexity')\n",
    "plt.xlabel('Messages to compare')\n",
    "plt.ylabel('Time to compute similarities (s)')\n",
    "plt.plot(times[0], times[1], label='Observations')\n",
    "x = np.array(times[0])\n",
    "plt.plot(x, estimate(x), label='Estimation using x²')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with (1) a brief commmentary about what you see in this plot, and (2) your estimate for how long it would take to run the brute force similarity computations for the entire input matrix. Express your estimation in hours, minutes, and seconds. Justify precisely your calculations.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By logic, comparing all the elements of two sets should be of time complexity O(n²). We can see that reflexted in the code, where we are using a nested for loop. If we try to fit a similar function to the graph and normalize it to the scale of the plot, we see that it matches almost perfectly. If we use that same function to compute how much time will it take to compare the entire input matrix (in the CovidLockdownCatalonia file there are 35.500 elements), we obtain the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.timedelta(seconds=estimate(35500))\n",
    "hours, remainder = divmod(time, datetime.timedelta(hours=1))\n",
    "minutes, remainder = divmod(remainder, datetime.timedelta(minutes=1))\n",
    "seconds = remainder / datetime.timedelta(seconds=1)\n",
    "\n",
    "print('It will take about {} hours, {} minuntes and {:.1f} seconds ({})'.format(hours, minutes, seconds, (str(time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it might be even longer if the data that we are handling does exceed the capacities of our setup (e.g.: does not fit in RAM and has to be swapped to disk to be managed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Computing the doc-ngram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create list of all ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for creating the ngram_to_index dictionary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_SIZE = 4\n",
    "\n",
    "ngram_to_index = {}\n",
    "index_to_ngram = {}\n",
    "next_index = 0\n",
    "\n",
    "for message in messages:\n",
    "    all_ngrams = ngrams(message, NGRAM_SIZE)\n",
    "    for ngram in all_ngrams:\n",
    "        if ngram not in ngram_to_index:\n",
    "            ngram_to_index[ngram] = next_index\n",
    "            index_to_ngram[next_index] = ngram\n",
    "            next_index += 1\n",
    "            \n",
    "num_distinct_ngrams = next_index\n",
    "\n",
    "print(\"There are %d distinct ngrams in the %d documents\" % (num_distinct_ngrams, len(messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for testing the ngram_to_index structure.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['els restaurants han reduir', 'supera fins tot mateix']\n",
    "max_len = max([len(t) for t in test])\n",
    "\n",
    "for t in test:\n",
    "    idx = ngram_to_index[t]\n",
    "    t2 = index_to_ngram[idx]\n",
    "    print(f'(SUCCESS={t==t2}) ngram_to_index[{'\\''+t+'\\'':{max_len+2}}] = {idx}\\tindex_to_ngram[{idx:^4}]=\\'{t2}\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create table ngrams x documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE AS-IS\n",
    "\n",
    "# Create dense matrix in which every cell contains the value \"False\"\n",
    "M_ngram_doc = np.full((num_distinct_ngrams, len(messages)), False)\n",
    "\n",
    "# Print the number of rows and columns of this matrix\n",
    "# numpy.matrix.shape is a tuple, shape[0] is the number of rows, shape[1] the number of columns\n",
    "print(\"Matrix dimensions: %d rows (distinct shingles) x %d columns (distinct documents)\" % M_ngram_doc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for filling the M_ngram_doc matrix.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docid in range(len(messages)):\n",
    "    message = messages[docid]\n",
    "    all_ngrams = ngrams(message, NGRAM_SIZE)\n",
    "    for ngram in all_ngrams:\n",
    "        M_ngram_doc[ngram_to_index[ngram]][docid] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for printing the density of the M_ngram_doc matrix as a percentage. Use 4 decimals.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The density of the M_ngram_doc matrix is {:.4%}'.format(M_ngram_doc.sum()/M_ngram_doc.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for printing rows 9602 and 941 of the M_ngram_doc matrix.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [962, 941]\n",
    "for t in test:\n",
    "    clean_str = clean(messages[t])\n",
    "    nonz = M_ngram_doc[:,t].nonzero()[0]\n",
    "    test_ngrams = [index_to_ngram[i] for i in nonz]\n",
    "    print(f'{'-'*90}\\nPositions of non-zeros in column of docid {t} of M_ngram_doc\\n')\n",
    "    print(f'Clean text:\\n{clean_str}\\n')\n",
    "    print('Non-zeros in corresponding row:')\n",
    "    for i, ngr in zip(nonz, test_ngrams):\n",
    "        print(f'{i:^4} ({ngr})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implement a permutation generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"random_permutation\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_permutation(k: int):\n",
    "    rand_perm = list(range(k))\n",
    "    random.shuffle(rand_perm)\n",
    "    return rand_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE AS-IS\n",
    "\n",
    "# Permute a list according to a permutation\n",
    "def permuter(original_list, permutation):\n",
    "    permuted_list = []\n",
    "    for index in permutation:\n",
    "        permuted_list.append(original_list[index])\n",
    "    return permuted_list\n",
    "\n",
    "# Code for testing permutations\n",
    "original_list_1 = [\"1 (test1)\", \"2 (test2)\", \"3 (test3)\", \"4 (test4)\", \"5 (test5)\"]\n",
    "original_list_2 = [\"1 (alpha)\", \"2 (gamma)\", \"3 (beta)\", \"4 (delta)\", \"5 (epsilon)\"]\n",
    "\n",
    "print(\"Test one permutation:\")\n",
    "permutation_1 = random_permutation(5)\n",
    "print(permuter(original_list_1, permutation_1))\n",
    "print(permuter(original_list_2, permutation_1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Test another permutation\")\n",
    "permutation_2 = random_permutation(5)\n",
    "print(permuter(original_list_1, permutation_2))\n",
    "print(permuter(original_list_2, permutation_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Compute the signature of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "NUM_PERMUTATIONS = 5\n",
    "\n",
    "permutations = []\n",
    "\n",
    "# Create the permutations\n",
    "for i in range(NUM_PERMUTATIONS):\n",
    "    permutation = random_permutation(num_distinct_ngrams)\n",
    "    permutations.append(random_permutation(num_distinct_ngrams))\n",
    "    \n",
    "# Visualize the permutations by printing their first 4 elements\n",
    "for i in range(len(permutations)):\n",
    "    permutation = permutations[i]\n",
    "    print(\"Permutation %d: %d, %d, %d, %d, ...\" % (i,\n",
    "                permutation[0], permutation[1], permutation[2], permutation[3] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE AS-IS\n",
    "\n",
    "# Find the first ngram in a document, according to a permutation\n",
    "def find_first_one(docid, permutation):\n",
    "    for shingle_id in permutation:\n",
    "        if M_ngram_doc[shingle_id, docid] == True:\n",
    "            return shingle_id\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for creating M_signature_doc</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_signature_doc = np.full((NUM_PERMUTATIONS, len(messages)), np.nan)\n",
    "\n",
    "# Create permutations\n",
    "for permutation_num in range(NUM_PERMUTATIONS):\n",
    "    print(\"Creating signatures for permutation %d/%d\" % (permutation_num+1, NUM_PERMUTATIONS))\n",
    "    permutation = permutations[permutation_num]\n",
    "    for docid in range(len(messages)):\n",
    "        if docid % 1000 == 0:\n",
    "            print(\"- Scanning document %d of %d\" % (docid, len(messages)))\n",
    "        \n",
    "        # It is important to take into account messages that are smaller than NGRAM_SIZE\n",
    "        first_one = find_first_one(docid, permutation)\n",
    "        if first_one > -1:\n",
    "            M_signature_doc[permutation_num, docid] = first_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def extract_ngrams(docid):\n",
    "    return [x for x in range(num_distinct_ngrams) if M_ngram_doc[x, docid] == True]\n",
    "\n",
    "def extract_signature(docid):\n",
    "    return [M_signature_doc[x, docid] for x in range(NUM_PERMUTATIONS)]\n",
    "\n",
    "def print_sig(messages, M_ngram_doc, M_signature_doc, i):\n",
    "    print(\"Document #%d\" % i)\n",
    "    print(\"Message       : %s\" % messages[i])\n",
    "    print(\"Clean message : %s\" % clean(messages[i]))\n",
    "    print(\"Ngrams        : %s\" % extract_ngrams(i))\n",
    "    print(\"Signature     : %s\" % extract_signature(i))\n",
    "\n",
    "# Print two messages and their signatures\n",
    "\n",
    "print_sig(messages, M_ngram_doc, M_signature_doc, 21 )\n",
    "print()\n",
    "print_sig(messages, M_ngram_doc, M_signature_doc, 24 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compare all pairs of signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for comparing all signatures; print all documents that have at least X signature matches, considering both full matches and partial matches. X should be 0.5% of the  number of documents.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_possible_duplicate = {}\n",
    "possible_duplicate_log = {}\n",
    "relevants = []\n",
    "\n",
    "# Iterate through all documents\n",
    "for docid1 in range(len(messages)):\n",
    "\n",
    "    # Do not examine again a document that is a possible duplicate\n",
    "    if docid1 not in is_possible_duplicate:\n",
    "\n",
    "        # Counters for full and partial signature matches\n",
    "        count_sig_full_matches = 0\n",
    "        count_sig_partial_matches = 0\n",
    "\n",
    "        # Extract the signature of the doc1\n",
    "        signature1 = extract_signature(docid1)\n",
    "        if docid1 % 500 == 0:\n",
    "            print(\"%d/%d documents scanned\" % (docid1, len(messages)))\n",
    "\n",
    "        # Iterate through documents with docid larger than doc1\n",
    "        for docid2 in range(docid1+1, len(messages)):\n",
    "\n",
    "            # If this has not already been marked as duplicate of another document\n",
    "            if docid2 not in is_possible_duplicate:\n",
    "\n",
    "                # Extract signature of doc2\n",
    "                signature2 = extract_signature(docid2)\n",
    "\n",
    "                # Compute the jaccard similarity of both signatures\n",
    "                similarity = jaccard_similarity(signature1, signature2)\n",
    "\n",
    "                # If the similarity score is low, don't do anything\n",
    "                if similarity < 0.2:\n",
    "                    continue\n",
    "\n",
    "                # We have found a possible duplicate, add it so we don't find repeated entries\n",
    "                is_possible_duplicate[docid2] = docid1 # strange choice to use dict here since value is never accessed\n",
    "\n",
    "                # Dictionary to log all possible duplicates for storing results\n",
    "                if docid1 not in possible_duplicate_log:\n",
    "                    possible_duplicate_log[docid1] = []\n",
    "                possible_duplicate_log[docid1].append(docid2)\n",
    "\n",
    "                # Increase counters accordingly\n",
    "                if similarity == 1:\n",
    "                    count_sig_full_matches += 1\n",
    "                elif similarity >= 0.2:\n",
    "                    count_sig_partial_matches += 1\n",
    "\n",
    "        # If docid1 has relevant matches, store for testing and print\n",
    "        if count_sig_full_matches + count_sig_partial_matches > 0.005 * len(messages):\n",
    "            print(f'Document {docid1} has {count_sig_full_matches} full matches and {count_sig_partial_matches} partial matches')\n",
    "            relevants.append(docid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_possible_duplicates = dict(zip(relevants, [possible_duplicate_log[i] for i in relevants]))\n",
    "print(relevant_possible_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in relevant_possible_duplicates.items():\n",
    "    print()\n",
    "    print(messages[key])\n",
    "    for v in value:\n",
    "        print(clean(messages[v]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary, based on the results above, about one tweet that has a substantial number of complete matches, but few partial matches. Include the full text of the original tweet. Comment on why you believe this tweet is not being changed much when copied or re-tweeted.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the matched tweets are from articles or broadcast messages form institutions (on how to act in some scenarios). Most of them have full matches. We can see that in the following broadcast message:\n",
    "\n",
    ">'RT @emergenciescat: Què puc fer i que no? FAQs del #coronavirus a 14 de març. si us plau, demanem difusió. https://t.co/D5HNxwYjwK'\n",
    "\n",
    "This message specifically (5077) has 176 full matches and 0 partial matches. In these types of messages it is highly important to be concise and to not modify the message that is being sent, so there is not much room for 'partial modification'. That is why I believe that this, and other similar messages, have such a low partial matches in comparison to the total matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary, based on the results above, about one tweet that has a substantial number of partial matches, but fewer complete matches. Include the full text of the original tweet and one near duplicate (that cannot be identical to the original tweet).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the tweets that have more partial matches are the ones that belong to certain 'trends', have some kind of slogan, and are not coming from an institutional source. We do not have many of them (due to the low threshold of 0.5%) but the one that resembles the most to this kind of message is the following:\n",
    "\n",
    "> RT @VilaWeb: [VÍDEO] Ortega Smith passejant per Madrid sense mascareta i amb la seva mare després del positiu de coronavirus https://t.co/M…\n",
    "\n",
    "Which is the message 73. It has 16 full matches and 35 partial matches. Another partial match is the following:\n",
    "\n",
    "> RT @RafaXambo: [VÍDEO] Ortega Smith passejant per Madrid sense màscara tot i tenir el coronavirus https://t.co/YUNCKRjQ63\n",
    "\n",
    "Which is message 148."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra exercise\n",
    "\n",
    "For more learning and extra points, compare what happens with 3 different ngram sizes (2-grams, 3-grams, 4-grams) in terms of the efficiency (speed) and effectiveness (accuracy). You can include plots for efficiency, and examples for effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change slightly so they can be used inside the near_duplicated_signatures function\n",
    "# Basically swap global variables by parameters\n",
    "def find_first_one(docid, permutation, M_ngram_doc):\n",
    "    for shingle_id in permutation:\n",
    "        if M_ngram_doc[shingle_id, docid] == True:\n",
    "            return shingle_id\n",
    "    return -1\n",
    "\n",
    "def extract_ngrams(docid, num_distinct_ngrams, M_ngram_doc):\n",
    "    return [x for x in range(num_distinct_ngrams) if M_ngram_doc[x, docid] == True]\n",
    "\n",
    "def extract_signature(docid, num_permutations, M_signature_doc):\n",
    "    return [M_signature_doc[x, docid] for x in range(num_permutations)]\n",
    "\n",
    "def time_brute_force_similarities(messages, limit, ngram_size, threshold=0.005):\n",
    "    if limit > len(messages):\n",
    "        raise ValueError(\"Limit should be less than or equal than the number of messages\")\n",
    "        \n",
    "    # Start a timer\n",
    "    possible_duplicate_log = {}\n",
    "    relevants = []\n",
    "\n",
    "    # Iterate through document identifiers\n",
    "    for docid1 in range(np.min([len(messages), limit])):\n",
    "\n",
    "        # Clean document 1 and extract ngrams\n",
    "        doc1 = clean(messages[docid1])\n",
    "        ngrams1 = ngrams(doc1, ngram_size)\n",
    "\n",
    "        count_sig_full_matches = 0\n",
    "        count_sig_partial_matches = 0\n",
    "\n",
    "        # Iterate through document identifiers larger than doc2\n",
    "        for docid2 in range(docid1+1, np.min([len(messages), limit])):\n",
    "                         \n",
    "            # Clean document 2 and extract ngrams\n",
    "            doc2 = clean(messages[docid2])\n",
    "            ngrams2 = ngrams(doc2, ngram_size)\n",
    "\n",
    "            # Compute similarity\n",
    "            similarity = jaccard_similarity(ngrams1, ngrams2)\n",
    "\n",
    "            if similarity < 0.2:\n",
    "                continue\n",
    "            \n",
    "            if docid1 not in possible_duplicate_log:\n",
    "                possible_duplicate_log[docid1] = []\n",
    "            possible_duplicate_log[docid1].append(docid2)\n",
    "            \n",
    "            if similarity == 1:\n",
    "                count_sig_full_matches += 1\n",
    "            elif similarity >= 0.2:\n",
    "                count_sig_partial_matches += 1\n",
    "        \n",
    "        if count_sig_full_matches + count_sig_partial_matches > threshold * limit:\n",
    "            relevants.append(docid1)\n",
    "\n",
    "    return possible_duplicate_log, relevants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_duplicates_signatures(ngram_size: int = 4, num_permutations: int = 5, threshold: float = 0.005):\n",
    "    '''\n",
    "    Compresses the algorithm that we have implemented in the notebook in one function\n",
    "    so we can test performance easily. \n",
    "    '''\n",
    "    # 1. Indexing ngrams\n",
    "    ngram_to_index = {}\n",
    "    index_to_ngram = {}\n",
    "    next_index = 0\n",
    "\n",
    "    for message in messages:\n",
    "        all_ngrams = ngrams(message, ngram_size)\n",
    "        for ngram in all_ngrams:\n",
    "            if ngram not in ngram_to_index:\n",
    "                ngram_to_index[ngram] = next_index\n",
    "                index_to_ngram[next_index] = ngram\n",
    "                next_index += 1\n",
    "                \n",
    "    num_distinct_ngrams = next_index\n",
    "\n",
    "    # 2. Creating the boolean M_ngram_doc matrix that contains (ngrams x documents)\n",
    "    M_ngram_doc = np.full((num_distinct_ngrams, len(messages)), False)\n",
    "\n",
    "    for docid in range(len(messages)):\n",
    "        message = messages[docid]\n",
    "        all_ngrams = ngrams(message, ngram_size)\n",
    "        for ngram in all_ngrams:\n",
    "            M_ngram_doc[ngram_to_index[ngram]][docid] = True\n",
    "\n",
    "    # 3. Create random permutations of the rows\n",
    "    permutations = []\n",
    "\n",
    "    for _ in range(num_permutations):\n",
    "        permutations.append(random_permutation(num_distinct_ngrams))\n",
    "\n",
    "    # 4. Fill M_signature_doc matrix with the first ngram (id) according to the permutations\n",
    "    M_signature_doc = np.full((num_permutations, len(messages)), np.nan)\n",
    "\n",
    "    for permutation_num in range(num_permutations):\n",
    "        permutation = permutations[permutation_num]\n",
    "        for docid in range(len(messages)):\n",
    "            \n",
    "            # It is important to take into account messages that are smaller than NGRAM_SIZE\n",
    "            first_one = find_first_one(docid, permutation, M_ngram_doc)\n",
    "            if first_one > -1:\n",
    "                M_signature_doc[permutation_num, docid] = first_one\n",
    "\n",
    "    # 5. Compare intelligently the signatures of all documents\n",
    "    is_possible_duplicate = []\n",
    "    possible_duplicate_log = {}\n",
    "    relevants = []\n",
    "\n",
    "    # Iterate through all documents\n",
    "    for docid1 in range(len(messages)):\n",
    "\n",
    "        # Do not examine again a document that is a possible duplicate\n",
    "        if docid1 not in is_possible_duplicate:\n",
    "\n",
    "            # Counters for full and partial signature matches\n",
    "            count_sig_full_matches = 0\n",
    "            count_sig_partial_matches = 0\n",
    "\n",
    "            # Extract the signature of the doc1\n",
    "            signature1 = extract_signature(docid1, num_permutations, M_signature_doc)\n",
    "\n",
    "            # Iterate through documents with docid larger than doc1\n",
    "            for docid2 in range(docid1+1, len(messages)):\n",
    "\n",
    "                # If this has not already been marked as duplicate of another document\n",
    "                if docid2 not in is_possible_duplicate:\n",
    "\n",
    "                    # Extract signature of doc2\n",
    "                    signature2 = extract_signature(docid2, num_permutations, M_signature_doc)\n",
    "\n",
    "                    similarity = jaccard_similarity(signature1, signature2)\n",
    "                    \n",
    "                    if similarity < 0.2:\n",
    "                        continue\n",
    "\n",
    "                    is_possible_duplicate.append(docid2)\n",
    "                    \n",
    "                    if docid1 not in possible_duplicate_log:\n",
    "                        possible_duplicate_log[docid1] = []\n",
    "                    possible_duplicate_log[docid1].append(docid2)\n",
    "                    \n",
    "                    if similarity == 1:\n",
    "                        count_sig_full_matches += 1\n",
    "                    elif similarity >= 0.2:\n",
    "                        count_sig_partial_matches += 1\n",
    "\n",
    "            if count_sig_full_matches + count_sig_partial_matches > threshold * len(messages):\n",
    "                relevants.append(docid1)\n",
    "    \n",
    "    return possible_duplicate_log, relevants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE SIGNATURES ALGORITHM\n",
    "# Set test parameters\n",
    "step = 5\n",
    "test_ngram_size = range(2,5)\n",
    "test_num_perm = range(5,31,step)\n",
    "\n",
    "# We will store the results here\n",
    "testing_time_results = np.zeros((len(test_ngram_size), len(test_num_perm)))\n",
    "logs = []\n",
    "relevant_logs = []\n",
    "\n",
    "for ngram_size, num_perm in itertools.product(test_ngram_size, test_num_perm):\n",
    "    start = timer()\n",
    "    possible_duplicate_log, relevants = near_duplicates_signatures(ngram_size, num_perm)\n",
    "    logs.append(possible_duplicate_log)\n",
    "    relevant_logs.append(relevants)\n",
    "    stop = timer()\n",
    "    testing_time_results[ngram_size-test_ngram_size[0], int((num_perm-test_num_perm[0])/step)] = stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM A BRUTE FORCE COMPARISON SO WE CAN COMPARE THE SIGNATURE ALGORITHM AGAINST BRUTE-FORCE RESULTS\n",
    "testing_time_results_bf = []\n",
    "logs_bf = []\n",
    "relevant_logs_bf = []\n",
    "\n",
    "for ngram_size in test_ngram_size:\n",
    "    start = timer()\n",
    "    possible_duplicate_log, relevants = time_brute_force_similarities(messages, 1000, ngram_size)\n",
    "    logs_bf.append(possible_duplicate_log)\n",
    "    relevant_logs_bf.append(relevants)\n",
    "    stop = timer()\n",
    "    testing_time_results_bf.append(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(s1:list, s2:list):\n",
    "    num = len(set(s1)&set(s2))\n",
    "    den1 = len(set(s2))\n",
    "    den2 = len(set(s1))\n",
    "\n",
    "    if den1 == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = num / den1\n",
    "\n",
    "    if den2 == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = num / den2\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE ACCURACY FOR ALL POSSIBLE NEAR DUPLICATES\n",
    "testing_accuracy_results_all = np.zeros((len(test_ngram_size), len(test_num_perm), 2))\n",
    "\n",
    "for ngram_size, num_perm in itertools.product(test_ngram_size, test_num_perm):\n",
    "    i = ngram_size - test_ngram_size[0]\n",
    "    j = int((num_perm - test_num_perm[0])/step)\n",
    "    # Tipically the logs using signatures are a subset of the ones using bruteforce\n",
    "    # It might happen to have a false positive using signatures, then it would not count in the accuracy\n",
    "    precision = recall = 0.0\n",
    "    for key in logs_bf[i]:\n",
    "        s1 = logs_bf[i][key]\n",
    "        s1.append(key)\n",
    "        if key in logs[i*len(test_num_perm)+j]:\n",
    "            s2 = logs[i*len(test_num_perm)+j][key]\n",
    "            s2.append(key)\n",
    "        else:\n",
    "            s2 = []\n",
    "        p, r = precision_recall(s1, s2)\n",
    "        precision += p\n",
    "        recall += r\n",
    "    precision /= len(logs_bf[i])\n",
    "    recall /= len(logs_bf[i])\n",
    "    testing_accuracy_results_all[i, j] = [precision, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE ACCURACY FOR ONLY RELEVANT NEAR DUPLICATES (ABOVE THRESHOLD)\n",
    "testing_accuracy_results_relevant = np.zeros((len(test_ngram_size), len(test_num_perm), 2))\n",
    "\n",
    "for ngram_size, num_perm in itertools.product(test_ngram_size, test_num_perm):\n",
    "    i = ngram_size - test_ngram_size[0]\n",
    "    j = int((num_perm - test_num_perm[0])/step)\n",
    "    # Tipically the logs using signatures are a subset of the ones using bruteforce\n",
    "    # It might happen to have a false positive using signatures, then it would not count in the accuracy\n",
    "    for key in logs_bf[i]:\n",
    "        if key in relevant_logs_bf[i]:\n",
    "            s1 = logs_bf[i][key]\n",
    "            s1.append(key)\n",
    "            if key in relevant_logs[i*len(test_num_perm)+j]:\n",
    "                s2 = logs[i*len(test_num_perm)+j][key]\n",
    "                s2.append(key)\n",
    "            else:\n",
    "                s2 = []\n",
    "            p, r = precision_recall(s1, s2)\n",
    "            precision += p\n",
    "            recall += r\n",
    "    precision /= len(logs_bf[i])\n",
    "    recall /= len(logs_bf[i])\n",
    "    testing_accuracy_results_relevant[i, j] = [precision, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_num_perm:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Ngrams')\n",
    "    ax1.set_ylabel('Time (s)', color=color)\n",
    "    ax1.plot(test_ngram_size, testing_time_results[:,int((i-test_num_perm[0])/step)], color=color, label='Time')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xticks(test_ngram_size)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "    ax2.set_ylabel('Precision and recall', color='black')\n",
    "    ax2.plot(test_ngram_size, testing_accuracy_results_relevant[:,int((i-test_num_perm[0])/step),0], color='tab:blue', label='Precision')\n",
    "    ax2.plot(test_ngram_size, testing_accuracy_results_relevant[:,int((i-test_num_perm[0])/step),1], color='tab:green', label='Recall')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    fig.suptitle(f'Ngram size vs speed and accuracy (#perm={i})')\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_ngram_size:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Permutations')\n",
    "    ax1.set_ylabel('Time (s)', color=color)\n",
    "    ax1.plot(test_num_perm, testing_time_results[i-test_ngram_size[0],:], color=color, label='Time')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xticks(test_num_perm)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "    ax2.set_ylabel('Precision and recall', color='black')\n",
    "    ax2.plot(test_num_perm, testing_accuracy_results_all[i-test_ngram_size[0],:,0], color='tab:blue', label='Precision')\n",
    "    ax2.plot(test_num_perm, testing_accuracy_results_all[i-test_ngram_size[0],:,1], color='tab:green', label='Recall')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    fig.suptitle(f'Ngram size vs speed and accuracy (#ngrams={i})')\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the above plots that even for a small number of permutations the increase in precision and recall increases in both cases. Ideally if the permutation count was the same as the number of ngrams, the precision and recall should end up at 1. For a fraction of the computational complexity we get a 0.04 increase in both precision and recall (looking at ngram_size=2 plot), going from 5 permutations to 30. That is quite significant taking into account the small scale of the permutations.\n",
    "\n",
    "We can also see that the accuracy (precision and recall) seems proportional to the number of ngrams, regardless of the permutation count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
