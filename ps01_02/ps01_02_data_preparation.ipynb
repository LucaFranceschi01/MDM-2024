{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 01+02: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists [spend a big chunk of their time preparing data](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/) and this is one of the first steps in any data mining project. This step is normally called **data preparation**.\n",
    "\n",
    "The processes of getting an initial understanding of a dataset and preparing it usually go hand-in-hand, and it is critical to perform them well to obtain valid results later. Plus, you can save time and effort by learning how to do proper data preparation.\n",
    "\n",
    "In this session, we will assume you just received a new dataset and need to do some initial steps with it:\n",
    "\n",
    "1) Exploratory Data Analysis\n",
    "\n",
    "* Calculate basis statistics as mean, median, variance, maximum and minimum\n",
    "* Look at distributions, identify outliers\n",
    "* Calculate correlations between variables\n",
    "\n",
    "2) Feature engineering:\n",
    "\n",
    "* Deal with missing values\n",
    "* Standardize all numerical columns\n",
    "* Convert categorical columns to dummy binary variables\n",
    "* Date and period management\n",
    "* Feature generation\n",
    "\n",
    "*Tip*: This process has several steps. It is tempting to maintain a single variable throughout the entire cleaning process, and do something like `x = x.step1()` then `x = x.step2()`. This will create problems for you because if you go back and re-execute a cell it might fail to operate on already transformed data. A better approach in cases like this where you do not have memory problems, is to do `x1 = x.step1()`, `x2 = x1.step2()` and so on, i.e., create a new variable after each transformation or set of transformations.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Luca Franceschi</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">luca.franceschi01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">4/10/2024</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. The dataset\n",
    "\n",
    "The dataset, contained in `device_db.csv` is a 10000 registers of mobile device purchases around 2019. **Each record in the dataset describes a customer that buys a new mobile telephone**. The attributes are defined as follows:\n",
    " \n",
    "0. PURCHASED_DEVICE: the mobile phone bought by the customer\n",
    "1. DEVICE_VALUE: the cost of the mobile phone bought by the customer\n",
    "2. LAST_DEVICE_DATE: the date of the previous mobile device purchase\n",
    "3. DATA\\_TRAFFIC\\_MONTH_(1..6): The Mbps of data traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n",
    "4. VOICE\\_TRAFFIC\\_MONTH_(1..6): The minutes of voice traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n",
    "5. BILLING\\_MONTH\\_(1..6): Billing (USD) in the month (-1...-6) paid by the customer previous to the mobile device purchase\n",
    "6. DEVICE\\_COST\\_MONTH_(1..6): Monthly cost (USD) associated to the mobile device finance in the month (-1...-6) paid by the customer previous to the mobile device purchase: proportion of owner-occupied units built prior to 1940\n",
    "7. LINE\\_ACTIVATION\\_DATE: Date of the activation of the mobile line by the customer\n",
    "8. MONTHS\\_LAST\\_DEVICE: Number of months of the previous mobile device\n",
    "9. DURATION\\_LINE: Number of months since the customer contracted the mobile line\n",
    "10. PREVIOUS\\_DEVICE\\_MODEL: Model of the previous mobile phone\n",
    "11. PREVIOUS\\_DEVICE\\_MANUF: Manufacturer of the previous mobile phone\n",
    "12. PREVIOUS\\_DEVICE\\_BRAND: Brand of the previous mobile phone\n",
    "\n",
    "This dataset will be used in next practices as recommendation engines.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) allows to us to have an understanding of the dataset from a stadistics perspective, i.e., data distribution and correlation between variables. This is crucial to select the most relevant variables for some purpose.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open the csv file contaning the data using separator \";\" and assign to a dataframe (use `read_csv` from the Pandas library).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE AS-IS\n",
    "\n",
    "input_dataset = pd.read_csv(\"device_db.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data types and simple statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(input_dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of creating a data frame. Above, we created it by reading a file, but one can also create a dataframe from scratch, using an array of dictionaries. Example:\n",
    "\n",
    "```python\n",
    "countries = []\n",
    "countries.append({'capital': 'Београд', 'country': 'Република Србија'})\n",
    "countries.append({'capital': 'Nairobi', 'country': 'Jamhuri ya Kenya'})\n",
    "countries_df = pd.DataFrame(countries, columns=['country', 'capital'])\n",
    "display(countries_df)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe named `column_type_df` containing the name of each column, its type and the number of distinct elements in that column. To iterate through the columns of dataframe `df`, use `for column in df.columns`; to determine the type of a column, use `df[column].dtype`; to retrieve the number of distinct elements of that column, use `df[column].nunique()`; to retrieve the size of a column, use `df[column].size`.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type = []\n",
    "for column in input_dataset.columns:\n",
    "    column_type.append({'name': column, 'type': input_dataset[column].dtype,\n",
    "                        'distinct': input_dataset[column].nunique(), 'size': input_dataset[column].size})\n",
    "column_type_df = pd.DataFrame(column_type, columns=['name', 'type', 'distinct', 'size'])\n",
    "display(column_type_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a **series** (column) from a dataframe you can reference an attribute by name, e.g., `input_dataset.DEVICE_VALUE` returns the series of all device values.\n",
    "\n",
    "On a series, you can use functions from [numpy](https://numpy.org/doc/) such as `np.mean`, `np.median`, `np.std`, `np.min` and `np.max`; meanings are self-explanatory. These functions have equivalents `np.nanmean`, `np.nanmedian`, and so on that ignore NaN (not-a-number) values.\n",
    "\n",
    "To display floats using two decimals, you can use:\n",
    "\n",
    "```python\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "```\n",
    "\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_descriptors = []\n",
    "\n",
    "for col in column_type_df[column_type_df.type=='float64'].name:\n",
    "    series = input_dataset[col]\n",
    "    basic_descriptors.append({'name': col, 'mean': np.nanmean(series), 'median': np.nanmedian(series), 'min': np.nanmin(series), 'max': np.nanmax(series)})\n",
    "\n",
    "basic_descriptors_df = pd.DataFrame(basic_descriptors, columns=['name', 'mean', 'median', 'min', 'max'])\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "display(basic_descriptors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` function can be used to describe a series. To invoke it simply do `input_dataset.DEVICE_VALUE.describe()`\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in column_type_df.name:\n",
    "    print(input_dataset[col].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset['DURATION_LINE'].size - input_dataset['DURATION_LINE'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decribe function makes also quartiles, not only the median (2nd quartile). Also includes standard deviation. The count function also works differently, since its value is, in our manual computation, `size - count(NaN)`. However the describe function does not take into account distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Inventory of device models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exploratory data analysis, it is very useful to do an **inventory** or **census** of the possible values of a variable. For us, a census will be a frequency table in which you show the possible values of a variable, and their frequency, in decreasing order of frequency.\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "pdm_frequency = pd.DataFrame(input_dataset.PREVIOUS_DEVICE_MODEL.value_counts(normalize=True))  \\\n",
    "    .reset_index(drop=False)                                                        \\\n",
    "    .rename(columns={'PREVIOUS_DEVICE_MODEL': 'Previous_Device_Model', 'proportion': 'Frequency'})\n",
    "\n",
    "display(pdm_frequency.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "pdb_frequency = pd.DataFrame(input_dataset.PREVIOUS_DEVICE_BRAND.value_counts(normalize=True))  \\\n",
    "    .reset_index(drop=False)                                                    \\\n",
    "    .rename(columns={'PREVIOUS_DEVICE_BRAND': 'Previous_Device_Brand', 'proportion': 'Frequency'})\n",
    "\n",
    "display(pdb_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be the case that the market cap of Samsung is greater than the Apple one because they have more device models, however they might not have a single device that was particularly well received by the market.\n",
    "\n",
    "However it could also be that there is a ton of missing data regarding these columns that lead to this frequencies (e.g.: Samsung Previous_Device_Brand could always written whereas Apple products might be mostly NaN for some reason)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of extracting valuable features from the data. This requires pre-processing, combining, normalizing, and performing other operations on the values of some features.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Missing values management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not A Number** (NaN) is a generic term to refer to *something that should be a number, but is not*. Usually, the value is either missing completely (\"null\") or contains the wrong type of object, such as a string or a concept such as infinity.\n",
    "\n",
    "To find which columns contain NaN values, you can use the [isna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html) function, as explained, e.g., [here](https://medium.com/dunder-data/finding-the-percentage-of-missing-values-in-a-pandas-dataframe-a04fa00f84ab). \n",
    "\n",
    "To display a column as percentages in a dataframe, you can use:\n",
    "\n",
    "```python\n",
    "df['column_name'] = df['column_name'].map('{:,.2%}'.format)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "def display_NaN_Frequency(dataset):\n",
    "    nan_info = []\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        nan_info.append({'Column_Name': col, 'NaN_Frequency': dataset[col].isna().sum()/dataset[col].size})\n",
    "\n",
    "    nan_info_pd = pd.DataFrame(nan_info, columns=['Column_Name', 'NaN_Frequency'])\n",
    "    nan_info_pd = nan_info_pd[nan_info_pd.NaN_Frequency > 0].sort_values(by='NaN_Frequency', ascending=False)\n",
    "\n",
    "    display(nan_info_pd) # in this case all columns contain at least one NaN value\n",
    "\n",
    "display_NaN_Frequency(input_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way **NaNs** are managed varies according to the meaning of each variable. In some occasions, registers should be removed, filled with other columns or calculated (imputed).\n",
    "\n",
    "* To delete all rows containing a null value, we can use [dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "* To replace null values, we can use [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
    "\n",
    "Please note that these steps should be applied sequentially, i.e., the output of one step should be fed into the next step. You can do, for instance: `df02 = df01.operation(...)` followed by `df03 = df02.operation(...)` and so on.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Initially we have {input_dataset.shape[0]} rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = input_dataset.dropna(axis=0, subset=['PURCHASED_DEVICE', 'DEVICE_VALUE', 'PREVIOUS_DEVICE_MODEL'], how='any').copy()\n",
    "f'Now we have {df02.shape[0]} rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "df03 = df02.copy()\n",
    "for i in range(1,7):\n",
    "    df03.fillna({f'DATA_TRAFFIC_MONTH_{i}': 0}, inplace=True)\n",
    "    df03.fillna({f'VOICE_TRAFFIC_MONTH_{i}': 0}, inplace=True)\n",
    "    df03.fillna({f'BILLING_MONTH_{i}': 0}, inplace=True)\n",
    "    df03.fillna({f'DEVICE_COST_MONTH_{i}': 0}, inplace=True)\n",
    "\n",
    "# check NaN_Frequency\n",
    "display_NaN_Frequency(df03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df04 = df03.fillna({'LINE_ACTIVATION_DATE': df03['LAST_DEVICE_CHANGE']}).copy()\n",
    "\n",
    "# check NaN_Frequency\n",
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "display_NaN_Frequency(df04)\n",
    "\n",
    "# looks good\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# display(df03[df03.LINE_ACTIVATION_DATE.isna()].head(3))\n",
    "# display(df04[df03.LINE_ACTIVATION_DATE.isna()].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df04.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `df` is a dataframe, `df.shape` contains a tuple with the number of rows and the number of columns of the data frame. You should now print something like this:\n",
    "\n",
    "```\n",
    "Rows in the original dataset: M\n",
    "Rows in the new dataset: N ((100*(M-N)/M)% less)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_orig = input_dataset.shape[0]\n",
    "r_new = df04.shape[0]\n",
    "\n",
    "print(f'Rows in the original dataset: {r_orig}')\n",
    "print('Rows in the new dataset: {} ({:.2%} less)'.format(r_new, (r_orig-r_new)/r_orig))\n",
    "\n",
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "display_NaN_Frequency(df04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Distributions, outliers, and correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the distributions of some variables and apply some transformations.\n",
    "\n",
    "* You can use [Seaborn library](https://seaborn.pydata.org/) with `kde=False` to create a histogram.\n",
    "* You can use [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) with `kind='box'` to create a boxplot.\n",
    "\n",
    "Remember to include a title, x-axis label, and y-axis label. All of your plots delivered throughout the course should include these elements. Example:\n",
    "\n",
    "```\n",
    "ax = sns.histplot(...)\n",
    "ax.set(title=..., xlabel=..., ylabel=...)\n",
    "```\n",
    "    \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='DEVICE_VALUE Histogram', xlabel='DEVICE_VALUE (USD)', ylabel='Count')\n",
    "sns.histplot(df04.DEVICE_VALUE, kde=False, ax=ax, bins=50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device value seems to have an exponential decrease (from price $0 to around $2000), however there is an important amount of devices that seem to be normally distributed with cost around $3000. It may count as bimodal but I'm not quite sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='DURATION_LINE Histogram', xlabel='DURATION_LINE (months)', ylabel='Count')\n",
    "sns.histplot(df04.DURATION_LINE, kde=False, ax=ax, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duration line histogram seems to be exponentially decreasing and also seems bimodal (peaks at bins 20 and 50, and a valley at around 40)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see better these histograms when comparing them, you can use:\n",
    "\n",
    "```\n",
    "sns.histplot(data=..., bins=20, fill=False)\n",
    "```\n",
    "\n",
    "To use logarithmic scale on the X axis or the Y axis, you can use `plt.xscale('log')` or `plt.yscale('log')`.\n",
    "    \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='TRAFFIC_MONTH Comparison', xlabel='Minutes of Traffic', ylabel='Count')\n",
    "sns.histplot(df04.VOICE_TRAFFIC_MONTH_1, kde=False, ax=ax, binwidth=100, label='Month -1')\n",
    "sns.histplot(df04.VOICE_TRAFFIC_MONTH_6, kde=False, ax=ax, binwidth=100, color='orange', label='Month -6')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both distributions seem to be exponentially decreasing and unimodal, however there seems to be more traffic as time passes. In other words, the nearest the mobile purchase the more voice traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='BILLING_MONTH Comparison', xlabel='Billing amount (USD)', ylabel='Count')\n",
    "# sns.histplot(df04.BILLING_MONTH_1, kde=False, ax=ax, binwidth=50, label='Month -1')\n",
    "# sns.histplot(df04.BILLING_MONTH_6, kde=False, ax=ax, binwidth=50, color='orange', label='Month -6')\n",
    "\n",
    "# very ugly but it is a mess otherwise\n",
    "sns.histplot(df04.BILLING_MONTH_1, kde=False, ax=ax, bins=range(-150, 1000, 50), label='Month -1')\n",
    "sns.histplot(df04.BILLING_MONTH_6, kde=False, ax=ax, bins=range(-150, 1000, 50), color='orange', label='Month -6')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both distributions seem to follow some kind of unimodal F-distribution since the right-tail is quite longer than the left-tail. There is not a noticeable difference between those billing periods in my opinion. The only remark that could be important is that the month previous to the purchase of a mobile phone some clients were charged negatively (money returned). This could affect the decision of buying a new device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables having exponential distribution can be processed and visualized better after transforming them, usually by applying the `log(x+1)` function (we want to avoid zeros, hence the +1).\n",
    "    \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_transformed = np.log(df04.VOICE_TRAFFIC_MONTH_1 + 1)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='log(BILLING_MONTH_1+1) Histogram', xlabel='Billing amount (USD)', ylabel='Count')\n",
    "sns.histplot(voice_transformed, kde=False, ax=ax, bins=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the x-axis was now skrunk to a much more manageable range. All the extreme values (that could be outliers) are much more condensed in the right bins. An interesting byproduct of this transformation is that all the negative values are mapped to 0, thus creating a peak in the first bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "fig.suptitle('Boxplots')\n",
    "\n",
    "axs[0].set(ylabel='Mbps')\n",
    "axs[1].set(ylabel='Minutes')\n",
    "axs[2].set(ylabel='USD')\n",
    "\n",
    "df04.boxplot('DATA_TRAFFIC_MONTH_6', ax=axs[0])\n",
    "df04.boxplot('VOICE_TRAFFIC_MONTH_6', ax=axs[1])\n",
    "df04.boxplot('BILLING_MONTH_6', ax=axs[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to not show outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a lot of outliers in all cases, all of them are \"upper\" outliers. I would identify as outliers all data points that exceed: 60000 Mbps in the DATA_TRAFFIC_MONTH_6 boxplot, 1000 minutes in the VOICE_TRAFFIC_MONTH_6, and around $600 for BILLING_MONTH_6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are many dependencies between different attributes, e.g., a large voice traffic will probably be associated with a large data traffic, a more expensive bill, and possibly a more expensive device (`DEVICE_VALUE`).\n",
    "\n",
    "You can use [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to compute a correlation matrix, and [matplotlib.pyplot.matshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.matshow.html) to show this graphically.\n",
    "\n",
    "To compute Pearson correlations, you use:\n",
    "\n",
    "```python\n",
    "df.corr(method='pearson', numeric_only=True)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to calculate the correlation between all traffic attributes (i.e., voice and data), duration line, billing, device cost and device value. Display the result as a table with rows and columns corresponding to columns, and cells indicating correlations. Display the result as an image using ``matshow``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df04.corr(method='pearson', numeric_only=True)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr)\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "ax.xaxis.set_tick_params(which='both', top=False, labeltop=False, bottom=True, labelbottom=True)\n",
    "ax.set_xticks(range(0, corr.shape[0]), column_type_df[column_type_df.type == 'float64'].name.to_list(), rotation=360-55, ha='left')\n",
    "ax.set_yticks(range(0, corr.shape[0]), column_type_df[column_type_df.type == 'float64'].name.to_list())\n",
    "\n",
    "fig.suptitle('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary on the results. Is the billing more correlated, in general, with the data traffic or with the voice traffic?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many interesting correlations in this plot. The first thing that I've noticed is that all variables that have an index 1-6 are very positively correlated between them (all of them near the diagonal). Also there is a very negative correlation between MONTHS_LAST_DEVICE and LAST_DEVICE_CHANGE (the smaller the buy date, the larger the phone age), and also between DURATION_LINE and LINE_ACTIVATION_DATE (for the same reason).\n",
    "\n",
    "It seems that the billing is, in general, more correlated to data traffic than to voice traffic (the color of the block 1-6 is brighter for data traffic than for voice traffic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Date management and period calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will determine the date of the `LAST_DEVICE_CHANGE` of the last device that was changed in the entire dataset (i.e., the maximum value of the `LAST_DEVICE_CHANGE` column, plus 30 days). We will refer to that date as `latest_change`.\n",
    "\n",
    "Note that `LAST_DEVICE_CHANGE` is expressed as a floating point number in the format `YYYYMMDD.0`, for instance 3 of July of 2018 would be `20180703.0`. Convert to integer first, then to string.\n",
    "\n",
    "As a string, this is formatted according to [strptime](https://www.geeksforgeeks.org/python-datetime-strptime-function/) conventions with format `%Y%m%d`.\n",
    "\n",
    "Use [datetime.datetime.strptime](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) to convert to create object `latest_change` and print it.\n",
    "\n",
    "Next, add 30 days to that date to obtain object `now` (we will assume we are doing this processing 30 days after the latest device change). Use a `datetime.timedelta` object for that.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "```\n",
    "2019-05-01 00:00:00\n",
    "2019-05-31 00:00:00\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to create and print `latest_change` and `now`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_change = datetime.datetime.strptime(str(int(df04['LAST_DEVICE_CHANGE'].max())), '%Y%m%d')\n",
    "now = latest_change + datetime.timedelta(30)\n",
    "print(latest_change)\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, obtain the series corresponding to the last device change, you can do it by using [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) as if you were using `strptime`:\n",
    "\n",
    "```\n",
    "series_converted = pd.to_datetime(dataframe[column_name], format='%Y%m%d')\n",
    "```\n",
    "\n",
    "Now compute the difference between the now and the series_converted. \n",
    "\n",
    "Divide that difference by `30 * datetime.timedelta(days=1)` to obtain the difference in periods of 30 days (approximately one month).\n",
    "\n",
    "Replace the `MONTHS_LAST_DEVICE` column with those differences. You may need to [fill the NaN with zeroes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html), and [convert to type](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) `int`.\n",
    "\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code that replaces the **MONTHS_LAST_DEVICE** column to be equal to the difference, in periods of 30 days, between **LAST_DEVICE_CHANGE** and the `now` variable.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_converted = pd.to_datetime(df04['LAST_DEVICE_CHANGE'], format='%Y%m%d')\n",
    "series_converted = (now - series_converted) / (30 * datetime.timedelta(days=1))\n",
    "\n",
    "df05 = df04.copy()\n",
    "df05['MONTHS_LAST_DEVICE'] = series_converted\n",
    "df05.fillna({'MONTHS_LAST_DEVICE': 0}, inplace=True)\n",
    "df05 = df05.astype({'MONTHS_LAST_DEVICE': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to update the **DURATION_LINE** value to be the difference, in days, between **LINE_ACTIVATION_DATE** and the `now` variable. Indicate the average of **DURATION_LINE** -- what is that in years, approximately?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_converted2 = pd.to_datetime(df04['LINE_ACTIVATION_DATE'], format='%Y%m%d')\n",
    "series_converted2 = now - series_converted2\n",
    "\n",
    "df06 = df05.copy()\n",
    "df06['DURATION_LINE'] = series_converted2\n",
    "print(f'The average line duration is about {df06['DURATION_LINE'].mean().days / 365:.2f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4. Standarization and scaling of numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling a series involves changing the values. Standardization involves ensuring that the mean is 0 and the standard deviation is 1, while min-max scaling requires that the maximum is 1, the minimum is 0, and all remaining values are linearly interpolated.\n",
    "\n",
    "You can use [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standarize a variable, and [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to perform min-max scaling.\n",
    "\n",
    "The following example shows how to use these:\n",
    "\n",
    "```python\n",
    "test_data = [{'x': -1.0}, {'x': 2.0}, {'x': 3.0}, {'x': 6.0}]\n",
    "test_df = pd.DataFrame(test_data)\n",
    "display(test_df)\n",
    "\n",
    "test_df['x_standardized'] = StandardScaler().fit_transform(test_df[['x']])\n",
    "test_df['x_minmaxscaled'] = MinMaxScaler().fit_transform(test_df[['x']])\n",
    "display(test_df)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to standardize and min-max scale the **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1**, **BILLING_MONTH_1** and **DEVICE_COST_MONTH_1** columns. Save the results in new colums with the same name followed by **_STANDARD** and **_MINMAX** (e.g., DATA\\_TRAFFIC\\_MONTH\\_1\\_STAND, DATA\\_TRAFFIC\\_MONTH\\_1\\_MINMAX). Plot a histogram for each new variable.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df07 = df06.copy()\n",
    "\n",
    "df07['VOICE_TRAFFIC_MONTH_1_STANDARD'] = StandardScaler().fit_transform(df07[['VOICE_TRAFFIC_MONTH_1']])\n",
    "df07['BILLING_MONTH_1_STANDARD'] = StandardScaler().fit_transform(df07[['BILLING_MONTH_1']])\n",
    "df07['DEVICE_COST_MONTH_1_STANDARD'] = StandardScaler().fit_transform(df07[['DEVICE_COST_MONTH_1']])\n",
    "\n",
    "df07['VOICE_TRAFFIC_MONTH_1_MINMAX'] = MinMaxScaler().fit_transform(df07[['VOICE_TRAFFIC_MONTH_1']])\n",
    "df07['BILLING_MONTH_1_MINMAX'] = MinMaxScaler().fit_transform(df07[['BILLING_MONTH_1']])\n",
    "df07['DEVICE_COST_MONTH_1_MINMAX'] = MinMaxScaler().fit_transform(df07[['DEVICE_COST_MONTH_1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(14,12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.histplot(df07.VOICE_TRAFFIC_MONTH_1_STANDARD, kde=False, ax=axs[0], bins=40)\n",
    "sns.histplot(df07.BILLING_MONTH_1_STANDARD, kde=False, ax=axs[1], bins=40)\n",
    "sns.histplot(df07.DEVICE_COST_MONTH_1_STANDARD, kde=False, ax=axs[2], bins=40)\n",
    "sns.histplot(df07.VOICE_TRAFFIC_MONTH_1_MINMAX, kde=False, ax=axs[3], bins=40)\n",
    "sns.histplot(df07.BILLING_MONTH_1_MINMAX, kde=False, ax=axs[4], bins=40)\n",
    "sns.histplot(df07.DEVICE_COST_MONTH_1_MINMAX, kde=False, ax=axs[5], bins=40)\n",
    "\n",
    "fig.suptitle('Histograms of standardized variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Convert categorical columns to dummy binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables usually need to be transformed into numerical values to apply some machine learning methods.\n",
    "\n",
    "Use [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform a categorical variable to integer values. Example:\n",
    "\n",
    "```python\n",
    "colors_data = [{'color': 'Blue'}, {'color': 'Red'}, {'color': 'Orange'},\n",
    "               {'color': 'Blue'}, {'color': 'Orange'}, {'color': 'Blue'}]\n",
    "colors_df = pd.DataFrame(colors_data, columns=['color'])\n",
    "\n",
    "colors_df['colors_int_encoded'] = LabelEncoder().fit_transform(colors_df['color'])\n",
    "display(colors_df)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Create variable **PREVIOUS_DEVICE_BRAND_INT_ENCODED** containing an integer encoding of variable **PREVIOUS_DEVICE_BRAND**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08 = df07.copy()\n",
    "df08['PREVIOUS_DEVICE_BRAND_INT_ENCODED'] = LabelEncoder().fit_transform(df08['PREVIOUS_DEVICE_BRAND'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to convert a categorical variable to multiple columns using one-hot encoding. Example:\n",
    "\n",
    "```\n",
    "colors_data = [{'color': 'Blue'}, {'color': 'Red'}, {'color': 'Orange'},\n",
    "               {'color': 'Blue'}, {'color': 'Orange'}, {'color': 'Blue'}]\n",
    "colors_df = pd.DataFrame(colors_data, columns=['color'])\n",
    "\n",
    "color_dummies = pd.get_dummies(colors_df['color'], prefix='color_')\n",
    "colors_df_with_dummies = colors_df.join(color_dummies)\n",
    "display(colors_df_with_dummies)\n",
    "```\n",
    "\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to convert **PREVIOUS_DEVICE_MANUF** to dummy binary variables.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08_dummies = df08.join(pd.get_dummies(df08['PREVIOUS_DEVICE_MANUF'], prefix='manuf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current dataset we have a historic of 6 months for data traffic, voice traffic, billing and device cost. Feature generation consists of creating new attributes from the current dataset that can help us to create, e.g., better predictive models.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to create from the 6 months of **DATA_TRAFFIC\\_MONTH\\_[1-6]**, **VOICE_TRAFFIC\\_MONTH\\_[1-6]**, **BILLING\\_MONTH\\_[1-6]** and **DEVICE_COST\\_MONTH\\_[1-6]**, new columns with the mean, maximum, minimum, range (i.e., difference between maximum and minimum) for each element. For instance, column **DATA_TRAFFIC_MEAN** should contain the average of these six numbers: **DATA_TRAFFIC_MONTH_1**, **DATA_TRAFFIC_MONTH_2**, ..., **DATA_TRAFFIC_MONTH_6**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df09 = df08_dummies.copy()\n",
    "\n",
    "df09['DATA_TRAFFIC_MONTH_MEAN'] = df09[['DATA_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].mean(axis=1)\n",
    "df09['DATA_TRAFFIC_MONTH_MAX'] = df09[['DATA_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].max(axis=1)\n",
    "df09['DATA_TRAFFIC_MONTH_MIN'] = df09[['DATA_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].min(axis=1)\n",
    "df09['DATA_TRAFFIC_MONTH_RANGE'] = df09['DATA_TRAFFIC_MONTH_MAX'] - df09['DATA_TRAFFIC_MONTH_MIN']\n",
    "\n",
    "df09['VOICE_TRAFFIC_MONTH_MEAN'] = df09[['VOICE_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].mean(axis=1)\n",
    "df09['VOICE_TRAFFIC_MONTH_MAX'] = df09[['VOICE_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].max(axis=1)\n",
    "df09['VOICE_TRAFFIC_MONTH_MIN'] = df09[['VOICE_TRAFFIC_MONTH_' + str(i) for i in range(1,7)]].min(axis=1)\n",
    "df09['VOICE_TRAFFIC_MONTH_RANGE'] = df09['VOICE_TRAFFIC_MONTH_MAX'] - df09['VOICE_TRAFFIC_MONTH_MIN']\n",
    "\n",
    "df09['BILLING_MONTH_MEAN'] = df09[['BILLING_MONTH_' + str(i) for i in range(1,7)]].mean(axis=1)\n",
    "df09['BILLING_MONTH_MAX'] = df09[['BILLING_MONTH_' + str(i) for i in range(1,7)]].max(axis=1)\n",
    "df09['BILLING_MONTH_MIN'] = df09[['BILLING_MONTH_' + str(i) for i in range(1,7)]].min(axis=1)\n",
    "df09['BILLING_MONTH_RANGE'] = df09['BILLING_MONTH_MAX'] - df09['BILLING_MONTH_MIN']\n",
    "\n",
    "df09['DEVICE_COST_MONTH_MEAN'] = df09[['DEVICE_COST_MONTH_' + str(i) for i in range(1,7)]].mean(axis=1)\n",
    "df09['DEVICE_COST_MONTH_MAX'] = df09[['DEVICE_COST_MONTH_' + str(i) for i in range(1,7)]].max(axis=1)\n",
    "df09['DEVICE_COST_MONTH_MIN'] = df09[['DEVICE_COST_MONTH_' + str(i) for i in range(1,7)]].min(axis=1)\n",
    "df09['DEVICE_COST_MONTH_RANGE'] = df09['DEVICE_COST_MONTH_MAX'] - df09['DEVICE_COST_MONTH_MIN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code create an additional column **DEVICE_COST_TO_BILLING_RATIO** containing the ratio between **DEVICE_COST_MEAN** and **BILLING_MEAN** and plot its distribution.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df09.copy()\n",
    "\n",
    "df10['DEVICE_COST_TO_BILLING_RATIO'] = df10['DEVICE_COST_MONTH_MEAN'] / df10['BILLING_MONTH_MEAN']\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "ax.set(title='DEVICE_COST_TO_BILLING_RATIO Histogram', xlabel='', ylabel='Count')\n",
    "sns.histplot(df10['DEVICE_COST_TO_BILLING_RATIO'], kde=False, ax=ax, binwidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary on the distribution of the variable **DEVICE_COST_TO_BILLING_RATIO**. Can you recognize its distribution?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be an exponentially decreasing distribution, however it has a very long right-tail, thus making this histogram look very weird. We could probably get a better visualization by plotting the `log(x+1)` histogram. What this plot is basically telling us is that, for most of the entries of the dataset, either the device cost is very low or the billing cost is very high (in comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Text parsing/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, text processing is a very useful tool that can be used to improve datasets. In some use cases, for instance customer care applications using digital channels as Whatsapp, Facebook, etc..., data scientist teams mainly work with text data.\n",
    "\n",
    "One of the text processing technique is to extract concrete words or tokens from a sentence or documents. Regular expressions are a great tool to extract data trough these patterns.\n",
    "\n",
    "In this dataset, note that **PURCHASED_DEVICE** is a variable that is formed by a \"**device_code**\"+\"**_**\"+\"**manufacture name**\"+\"**  **\"+\"**device model**\". We want to split this variable into its components.\n",
    "\n",
    "Tip: use [str.split](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to separate a string into several parts.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to use the **PURCHASED_DEVICE** variable to create 3 new columns with the following variables names: **PURCHASED_DEVICE_CODE**, **PURCHASED_DEVICE_MANUFACTURER** and **PURCHASED_DEVICE_MODEL**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df10.copy()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df11['PURCHASED_DEVICE'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df11['PURCHASED_DEVICE'].str.extract(r'(.*)_(.*)\\*\\* \\*\\*(.*)', expand=True).head(5))\n",
    "tmp = df11['PURCHASED_DEVICE'].str.split('_| ', n=2, expand=True) \\\n",
    "        .rename(columns={0: 'PURCHASED_DEVICE_CODE', 1: 'PURCHASED_DEVICE_MANUFACTURER', 2: 'PURCHASED_DEVICE_MODEL'})\n",
    "\n",
    "# avoid doing the join operation multiple times\n",
    "if 'PURCHASED_DEVICE_CODE' not in df11.columns:\n",
    "    df11 = df11.join(tmp)\n",
    "\n",
    "display(df11[['PURCHASED_DEVICE_CODE', 'PURCHASED_DEVICE_MANUFACTURER', 'PURCHASED_DEVICE_MODEL']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to create two tables: one with the number of devices per manufacturer in **PURCHASED_DEVICE_MANUFACTURER** and one with the number of devices per manufacturer in  **PREVIOUS_DEVICE_MANUF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = df11['PURCHASED_DEVICE_MANUFACTURER'].value_counts().reset_index()\n",
    "table2 = df11['PREVIOUS_DEVICE_MANUF'].value_counts().reset_index()\n",
    "\n",
    "display(table1)\n",
    "display(table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Splitting and sampling a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting and sampling dataset are techniques that distribute the original dataset in n-parts. One of the most interesting application of these tools is to separate the dataset to train and test a machine learning model. Meanwhile sampling guarantees same type of data (i.e. distributions), splitting will separate the dataset with the ratio we need. Usually, 80%-20% or 70%-30% splitting ratios are the most common used.\n",
    "\n",
    "Once again, Sklearn library helps to us to cover this necessity through the function [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) which splits a dataset into two parts, which usually will be used for training and testing.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to split the dataset in two separate datasets: one with 70% of the rows and the other with 30% of rows</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train, test = train_test_split(df11, train_size=70, test_size=30, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to compute the main statistics (mean, standard deviation, min, max, 25%, 50%, 75%) for the variables **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1** and **BILLING_MONTH_1** in both training and testing parts of the dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "print('Train split:')\n",
    "display(train[['DATA_TRAFFIC_MONTH_1', 'VOICE_TRAFFIC_MONTH_1', 'BILLING_MONTH_1']].describe())\n",
    "\n",
    "print('Test split:')\n",
    "display(test[['DATA_TRAFFIC_MONTH_1', 'VOICE_TRAFFIC_MONTH_1', 'BILLING_MONTH_1']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating if you find these statistics match between the two splits, or do not match between them.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match more or less in most of things (we have to take into account that since it is a random split they might differ a bit from each other). However we see very noticeable differences in places where outliers play a big role, such as `min` or `max`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparing iPhone and Samsung J series users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, find some features that are different between users of an Apple iPhone and users of a Samsung J series phone (this includes J410G, J610G, J415G, and all other models by Samsung that start with a *J*).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to create two dataframes: one with all the attributes of Apple iPhone users and one with all the attributes of Samsung J series users.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with code to compare some variables between the two datasets. Consider 2 or 3 variables, plot together the histograms of each variable in both datasets (including a legend).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary on the differences you found between these two groups of users.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "Remember to read the section on \"delivering your code\" in the [course evaluation guidelines](https://github.com/chatox/data-mining-course/blob/master/upf/upf-evaluation.md).\n",
    "\n",
    "Deliver a zip file containing:\n",
    "\n",
    "* This notebook\n",
    "\n",
    "## Extra points available\n",
    "\n",
    "For more learning and extra points, remember what you learned in machine learning and create a simple [decision tree model](https://scikit-learn.org/stable/modules/tree.html) having as input variables:\n",
    "\n",
    "1. PREVIOUS\\_DEVICE\\_MODEL\n",
    "1. PREVIOUS\\_DEVICE\\_BRAND\n",
    "1. MONTHS\\_LAST\\_DEVICE\n",
    "\n",
    "And as output variable `PURCHASED_DEVICE_MANUFACTURER`. Measure the accuracy of this 3-variables model. Then, add two more variables, of your own choice, that improve the classification accuracy. Measure the accuracy of this 5-variables model.\n",
    "\n",
    "\n",
    "**Note:** if you go for the extra points, add ``<font size=\"+2\" color=\"blue\">Additional results: model purchased device</font>`` at the top of your notebook.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
